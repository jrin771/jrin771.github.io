<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-X3HG4WG8DR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-X3HG4WG8DR');
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>J's Blog</title>
  <style>
  body {
    display: flex;
    flex-direction: column;
    align-items: center;
  }

  main {
    max-width: 38rem;
    margin: 0 auto;
  }

  header h1, h2 {
    margin-top: 2.5rem;
  }

  nav {
    display: flex;
    justify-content: center;
    padding: 1rem;
  }

  nav a {
    padding-left: 1rem;
    font-weight: bold;
    text-decoration: underline;
    color: inherit;
  }
</style>
</head>
<body>
  <header>
    <h1>J's Blog</h1>
  </header>
  <nav>
    <a href="index.html">Home</a>
    <a href="blog.html">Posts</a> 
    <a href="HowToLandARocket.html">How To Land A Rocket</a>
  </nav>
  <main>   
    <article> 
      <h2> 4: Very Important People </h2> 
      <p>Internalizing lessons is hard. </p>
      <p>Oh sure, it's easy to hear what someone is saying, but to listen? </p>
      <p>To understand? </p> 
      <p>To (as my favorite TA Ahmed once said) feel it in your bones? </p>
      <p>That is difficult. </p>
      <p> "When you grow up you tend to get told the world is the way it is and you're life is just to live your life inside the world. Try not to bash into the walls too much. Try to have a nice family, have fun, save a little money. That's a very limited life. Life can be much broader once you discover one simple fact: Everything around you that you call life was made up by people that were no smarter than you and you can change it, you can influence it, you can build your own things that other people can use. Once you learn that, you'll never be the same again." - Steve Jobs </p>
      <p> You can just do things. </p>
      <p> So do them.</p>
      <p> Go outside and enjoy the weather. </p>
      <p> -J</p>
    <article>
    <article>  
      <h2> 3: Favorite Oscar Wilde Quote </h2> 
      <p> "Some cause happiness wherever they go; others whenever they go." </p> 
      <p> Parallelism ("whe_ever they go" ends both "halves" of the quote) </p> 
      <p> Pause in the middle: Comedic timing in writing (Somehow he can do this) </p> 
      <p> Subversion of expectations: This is going to be happy -> This is not very happy </p>  
      <p> Go outside and enjoy the weather. </p> 
      <p> -J</p>
    <article>
    <article>
      <h2> 2: Carmacking </h2> 
      <p> Carmacking is not a real word but as of today I’m making it one. </p>
      <p> Carmacking, as I call it, refers to legendary programmer John Carmack's current approach to creating AGI, Artificial General Intelligence. For those who don't know, AGI means a lot of things to a lot of people, so it's hard to give an exact definition (and is also why many academics have issues with it, but I choose to accept it as "a vibe rather than a lemma"). Still, it's roughly when a computer is as smart as a human. Again, this is vague, but how Carmack formulates it makes it less so: He thinks it's when a computer can roughly be equal to humans at the vast majority of digital tasks. Due to COVID, so much of the world has shown that it can be moved online, from SWE roles to Zoom meetings, so having digital systems that write business reports seems more likely than an AI superintelligence that destroys humanity. (Well, at least for now. I have more thoughts on that topic I'll write about another time. Maybe if we're stuck in an elevator together someday? (If you know where that is from, I'll pay you 20$, but you have to be right)) </p>
      <p> Anyways, Carmack thinks that the key (or actually keys) to AGI are going to be relatively simple and will seem obvious to us looking back, in the same way, that adding scale to get emergent effects (again, emergence is weird as a definition, but that's another time) seem obvious to us now. This then raises a question: Where are we going to get all of those ideas? </p>
      <p> Carmack's answer is that while we likely haven't discovered every piece, there are some clues in older papers from the 70s-80s starting days of AI to the 90s-early 00s winter-ish periods that might have suffered due to a lack of scale or modern computing. This has merit: The amount of AI research published is impossible for anyone to thoroughly read. When I see AI tools for organizing your AI tools about organizing AI papers, the task of "staying on top of things" feels hopeless. However, I tend to be very skeptical when I'm evaluating an old paper that may have actually been a gold mine if only it had been released in modern times. </p>
      <p> First, the central claim that many people who think that an 80s biophysics or cognitive science article will come in and revolutionize everything is that it could've worked if only there was scale. The typical example they use is artificial neural networks (neural nets), which were fine but didn't become superhuman at many narrow tasks until Toronto researchers made them really deep (lots of layers) and threw a boatload of GPUs at the problem. (Also, length of training time played a factor, but back in 2012, that wasn't nearly as appreciated as it is now.) 
</p> 
      <p> However, today's SOTA (State of the Art) AI systems are massive. They have hundreds of billions of parameters, and to say, "Oh, my algorithm scales well," means something entirely different at this level. Something people forget is that scaling wasn't always assumed to work: it was counterintuitive for most. Many ML/AI researchers back before LLMs and the transformer architecture took off the thought that there would be diminishing returns for scale after a while and that the emergent effects we're seeing wouldn't really exist. Due to the glory of Jensen Huang (and his lovely engineering basement named after him that I currently reside in) and the rise of GPUs for AI research getting better year after year, we've avoided this paradox. (for now)</p>
      <p> Second, I think there's a difference between being inspired by the brain and trying to directly copy what it does. Ideas such as backpropagation and dropout, now crucial to modern AI, are probably not what the brain does. There's more nuance to this argument, where yes, a plane doesn't fly by flapping its wings, but to build an airplane that worked, we had to study birds for centuries. Perhaps the issue isn't the brain itself but rather going too granular. Having systems such as "How can we handle language? Visual stimuli? Sense of itself in physical space? Etc?" could be interesting, but this is more musings from me since I am not a qualified neuroscientist or biologist.</p>
      <p> At the end of the day, this doesn't mean that I think Carmacking is wrong. On the contrary, I think it'll work and that even if it doesn't give us everything, we will find stuff that will be mind-blowing. Fortunately, humans haven't just started thinking about AI when ChatGPT got released, it's been a long ride, and the car hasn't stopped yet. Oh no, we're just starting. </p> 
      <p> Go outside and enjoy the weather. </p> 
      <p> -J </p>
      <article>
      
      
      
    <article>  
      <h2>1.1: A small collection (1) of previous bangers</h2> 
      <p>(To audience of Stanford students) Ok this should be common sense but y'all should actually enjoy your major. I'm a bit obsessive (like I read math papers/write transformers from scratch on Saturdays) but if u wanna die every day don't do it.</p>
      <p>This is your life and don't waste it on stuff that you don't even like. The privilege of a place like Stanford is that you can do (almost) anything. Your limit is your imagination and however much money you can steal from VCs.</p>
      <p>You'll make way more money if you like what you do (at least a little bit) than if you completely hate your job. Why? Exceptional talent is rewarded. A child who's main talent is doing unboxing videos makes more than most CEOs. There are multiple professional whistlers. If you're good you're good.</p>
      <p>Now going for passion as your sole determinant for "this is what I want to do with my life" is flawed (re. The Trouble With Passion (Cich)) but if you have the choice do something you don't absolutely despise. You'll have plenty of time for that when tax season comes around. (unless if you like doing taxes. If that is the case my email is jrin@stanford.edu)</p>
      <p>Go outside and enjoy the weather.</p>
      <p>-J</p>
    </article>
    <article>
      <h2>1: Robots that don't suck</h2>
      <p>Hey everyone, this is my first post on my new little website. I need to still add in stripe stuff and make it not look awful but let’s get down to something I’ve been obsessed with the past few days:</p>
      <p>LLMs are going to make robots not suck for once.</p>
      <p>Explain?</p>
      <p>Robots currently suck. There’s lots of reasons for this, but a big one is that we don’t speak the same language. Humans deal with messy, vague, intuitive language, while
      robots deal with precise, exact code.</p>
<p>A computer will do exactly what you tell it to do.</p>
<p>No more.</p>
<p>No less.</p>
<p>So then, how do we fix this problem?</p>
<p>For the longest time, we didn’t. People slapped a bunch of sensors onto stuff and tried some primitive CV models, but they weren’t sci-fi level.</p>
<p>Now we get to the good part: ChatGPT and GPT-4. Both of these LLMs blew away previously available consumer LLMs, and have rapidly been integrated into a bunch of different robots. While embodied models such as PaLM-E also are leading in the space, nobody outside of Google Research can actually use it, so ordinary people (like me) are going to have to work with OpenAI for now.</p>
<p>Now, the big workflow for how to actually use LLMs for robots is that they essentially just move you up a level of abstraction. Instead of telling the robot through code directly what to do, you tell the LLM to tell the robot through code to do what you want it to do. I haven’t seen anyone try testing code-specific LLMs on these sorts of tasks yet, but I wonder if they would end up being more efficient for this type of work.</p>
<p>However, while this is nice, the most immediate commercial use cases for LLMs in robotics will likely be as a nice UI for databases/robot information. Boston Dynamics was good with this for integrating ChatGPT into their SPOT dog robot and asking it questions such as “what is your battery percentage?” “How many thermal anomalies did you find 30 minutes ago?” “Is that menswear guy on Twitter near my house again please sir stop I’m scared” etc.</p>
<p>Some interesting directions beyond these two are that Microsoft Research used LLMs for drones (my friend Brian also did this for Treehacks, although not at the same scale due to hackathon limitations) and the University of Florida did a study where 15 people tried interacting with a robot through LLMs. (Spoiler alert: It’s actually really good and will probably get deployed in factories once guardrails have been deployed).</p>
<p>Oh yeah. Guardrails.</p>
<p>See, there’s problems with just blindly slapping LLMs into robots. It’s fun when it’s a party trick or helping factories become more efficient, but not when it gets integrated into police or military units. Hallucination is bad but to me prompt injection seems significantly more pressing. Do Anything Now (DAN) and other techniques could cause quite a bit of harm if they aren’t accounted for. Currently I don’t know of any good solutions besides mass RLHF and a very dedicated team of engineers, but if there’s any politicians (or other persons) in NYC that read this blog you should seriously consider this before it gets messy.</p>
<p>Anyways I have things (homework and laundry) to attend to, but this was nice.</p>
<p>Go outside and enjoy the weather.</p>
<p>-J</p>
</article>
  </main>
  <footer>
  </footer>
</body>
</html>
      
      
      
      
      
      
      
      
