<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-X3HG4WG8DR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-X3HG4WG8DR');
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>J's Blog</title>
  <style>
  body {
    display: flex;
    flex-direction: column;
    align-items: center;
  }

  main {
    max-width: 38rem;
    margin: 0 auto;
  }

  header h1, h2 {
    margin-top: 2.5rem;
  }

  nav {
    display: flex;
    justify-content: center;
    padding: 1rem;
  }

  nav a {
    padding-left: 1rem;
    font-weight: bold;
    text-decoration: underline;
    color: inherit;
  }
</style>
</head>
<body>
  <header>
    <h1>J's Blog</h1>
  </header>
  <nav>
    <a href="blog.html">Posts</a>
    <a href="index.html">About Me</a>
    <a href="EverythingLLMsAndRobotics.html">Everything LLMs And Robotics</a> 
    
  </nav>
  <main>    
    <article>   
      <article>  
        <h2>22: engineers *handshake* artists </h2> 
        <p> let's get the surface level comparisons out of the way. </p> 
        <p> both make things </p>
        <p> both have useless knowledge about weird subparts of the world </p>
        <p> both can be seen not eating, sleeping, or showering (depravity) </p>
        <p> Anyways, here's a deeper analysis </p>
        <p> Zuck still thinks of himself as an engineer despite not writing code these days, because he is building large scale systems (of people) </p>
        <p> Art is complicated because technical skill =/= capability in a way that engineering more directly has (it doesn't matter how much you care if you can't do math) </p> 
        <p> However, I think that the process of engineering is the process of coding is the process of thinking and doing that transcends any given medium </p>
        <p> Being a programmer is not about writing lines of code it's about creation which so happens to be channeled through a text-based medium with specific rules (take your pick of programming languages, but please don't say javascript) </p>
        <p> I've noticed this over the course of this year: while my training in mechanical engineering is virtually nonexistent, I've gotten much better at CAD and design and everything associated with mechanical engineering solely because of participating in other large engineering projects </p>
        <p> tl;dr the engineering part of mechanical or electrical or computer or (insert xyz) matters more than the name, at least in terms of what you're doing </p>
        <p> now, just like in art, you can't be a master of one medium and then assume that you'll perfectly get another medium. That falls into the software engineer and the physicist fallacy of believing every other field just isn't trying hard enough (hint: the only person not trying hard enough is you) </p>
        <p> Yet, I bet that picasso could've been one of the world's greatest photoshop artists relatively easily if he was 1) alive and 2) learned the medium </p>
        <p> This is the same with new programming languages. When I was younger and significantly less experienced (I can't say I am now but I do have more than before) I couldn't understand how people would say that they knew 10 programming languages. </p>
        <p> The answer is lying for most people, but for the ones that do genuinely "know" 10 programming languages what they really know is 1 logical engineering language which then can be molded onto any good turing complete PL (although this is not exactly how it works and having spent most of your time in one language means you think in it more, pg has many good essays on this) </p>
        <p> I'm going to revise this to be later but tl;dr (tl;dr) </p>
        <p> get the primitives down, the rest are just wrappers. (wow, yacine is so wise)</p>
        <p> Go outside and enjoy the weather </p>
        <p> - J </p>
      <article> 
      <article> 
      <h2>21: How to speedrun a Stanford Genetics PhD </h2>
      <p> Note: This is a purely academic exercise. </p>
      <p> This is not advised. </p>
      <p> However... </p>
      <p> if you were to speedrun a PhD in genetics at the Stanford medical school, here's how I would go about doing it. </p>
      <p> Identify what you actually need to get a PhD. This may seem obvious but there's a lot of busywork and "extra" stuff that you don't technically need to get a PhD.  </p>
      <p> The hard requirements from what I could find are: </p>
      <p> *Need to take at least 9 courses from the med school (with at least a B/B- average) </p>
      <p> *Minimum 60 hours of teaching and/or community outreach. Community outreach can take many forms, with science communication being one of them, so if you already have a blog or are good at writing this would not be terribly difficult to do in a year. </p>
      <p> *Must have at least 1 first author paper published. This was surprising as I thought there would be more pub requirements but no, as long as you've got at least 1 first author pub (ideally peer reviewed I would assume rather than just a preprint repository) then that's what you need. </p>
      <p> *Have your thesis approved. </p>
      <p> *Pass your oral exam/quals. </p>
      <p> *Pass your thesis defense. </p>
      <p> *Arrange your thesis defense.</p>
      <p> *Turn in your forms on time.   </p>
      <p> *Pay all of your bills/get your forms in order. </p>
      <p> Honestly, after looking at some of this list, getting 3 professors into the same room at the same time is probably harder than publishing a paper, especially if you're a computationalist who can quickly spin up some experiments on spare GPUs. I suspect that combined with the more physical focus of many genetics phd students (while stanford is quite good on the computational side you have to still do a lot of hands-on wet lab stuff which takes quite a bit of time, as anything that requires doing something physical does. (Although less time than some very complicated organic syntheses or other biosciences/sciences labs can take depending on your experiments)). </p>
      <p> A second challenge I could see would be waiting on journals for your first author publication. You don't need a Nature pub for a phd. Granted, if your goal is a prestigious academic career at a top research university, then a nature/cell/science paper(s) would look quite nice. I think computational journals tend to be much faster than traditional journals if only because the culture of anything computational is to move fast due to software being fast.  </p>
      <p> This probably is filled with many flaws, but is interesting to think about. </p>
      <p> Go outside and enjoy the weather. </p>
      <p> - J </p>
      <article>
      <article> 
        <h2> 20: A short list of flaws </h2>
        <p> Eye contact. I'm the worst at it. I've been called out on it multiple times. Idk why. I think it's zoom's fault, for allowing me to multitask, but even in person it hasn't gone away. I'm not sure what I should do. Perhaps I do it so that I can focus on ideas better but this is killing my personal presence. 
        <p> Talking over people/not letting them talk. This is one I've been getting better at (I measure it in the amount of uncomfortable gazes) in part by being more empathetic but also in part by actually being interesting enough that people don't care if I'm doing annoying stuff. PMF heals all wounds. 
        <p> Spending too much on uber eats. In my defense stanford dining has gone downhill in the last two weeks. It really has. Perhaps it's a conspiracy theory but I don't care. 
        <p> Not drinking enough water. This is something that's gone downhill. I used to drink a lot of water. Now I don't. I should do that more. I'm mostly water anyways. I don't drink other things like coffee. I just don't have any liquids. Well, liquid liquids. I have water from food but that's not enough. 
        <p> Sunburns. Not really a flaw as much of a "ow my chest hurts" comment. My skin was peeling today. It was weird. I felt like a snake. Jake the snake. It does rhyme.  
        <p>Go outside and enjoy the weather. </p> 
        <p> - J </p>    
      <article>
      <article>  
        <h2> 19: Embedding Everything </h2> 
        <p> Thought: will's arxiv project was very cool. Another thought: arxiv doesn't cover lots of bio/medicine/chem. Third thought: let's do this but for all the other arxiv variants. </p>
        <p> Problem: I don't know how to do this. Solution: Look at the code. Time passes. Wait. There's no code for creating the dataset. how did you do this? I didn't I just found it on kaggle. </p> 
        <p> Shit. </p> 
        <p> Tell me there's more datasets. There aren't. Well this is great. How do I get there to be more datasets. Loudness? Sure. *sidetracked by a conversation* posted. 2 likes. well, so much for that. </p> 
        <p> New idea. (not from me) openbioml has a lot of the chemrxiv files downloaded. build on that. release it. use that to build momentum to pressure them into doing stuff. do the same for cshl. very silicon valley lean startup vibes. </p> 
        <p> time to build. </p>
        <p>Go outside and enjoy the weather. </p> 
        <p> - J </p>
      <article>
      <article>  
        <h2> 18: Visualizing Exponentials Is Hard </h2> 
        <p> Today I was reading a short piece about the life of George Church, a phenomenal scientist that I would love to meet and perhaps work with some day, when he mentioned the slightly radical idea of the 1000$ genome. </p> 
        <p> Nowadays, as of May 24th, 2023, it costs around 200-600$ for a genome, with startups promising to hit 100$ for a genome soon. </p> 
        <p> Church's comment was less than ten years ago (in 2014) and this is during one of the "slower" periods for a drop in genome sequencing. During the late 2000s and early 2010s, price drops in genome sequencing were so rapid that they made Moore's Law look almost flat in comparison. </p> 
        <p> This is one of the many reasons I'm so fascinated by genetics. There is almost no other field (other than AI, which is an anamoly among anamolies) which has been able to grow so fast. (pun intended) </p> 
        <p> If you haven't already, Elliot Hershberg (Author of the Century of Biology Substack) has written many great resources about these exponential changes in genetics. </p> 
        <p> Go outisde and enjoy the weather. </p> 
        <p> - J </p>
      <article>
      <article> 
       <h2>17: Talking about Epigenetic Reprogramming</h2>
       <p>This is something I might write about in a longer form piece but we need better ways of talking about partial epigenetic reprogramming. </p>
       <p>CRISPR got a lot more hype back in the mid-2010s but now that aging biotech companies are starting to really get funded/moving I sense that there's going to be issues.   </p>
       <p>This is especially true with the recent raise for newLimit and Retro Bio (they're both very cool you should check out their research plans).</p>
       <p>Mainly, I see that there's currently a perception that all work in partial epigenetic reprogramming is about "cheating death" which is a very loaded thing to say about a scientific field, especially to a public which has <p>already shown from discussions about AI that it doesn't have a ton of nuance about esoteric nerd topics.</p>
       <p>My current thoughts are that because partial epigenetic reprogramming has a lot in common with CRISPR (yes they're very different technologies but, rhetorically, them both being recent biotech Nobel winning prizes that form controversial startups is as good of a comparison point as we'll ever get) analyzing where CRISPR outreach/rhetoric/etc. went right and wrong is the way to go.</p>
       <p>However, both of these technologies are still rapidly evolving (rapid might be too slow, how about blasting ahead) and implying that we ever fully "got" CRISPR rhetoric right is false. </p>
       <p>I forsee much reading in my near future.</p>
       <p>Go outside and enjoy the weather. </p>
       <p>- J </p>
     <article>
     <article>  
       <h2> 16: Energy Ramblings </h2> 
       <p> Energy rules everything around us. The more energy you have, the easier it is to get very cool technology such as carbon capture and desalination to work. </p> 
       <p> However, due to a lot of things, we don't have "energy too cheap to meter" at the moment. That puts a guardrail on a lot of things that we could be doing if only we had the power. </p> 
       <p> Compute: Digital World :: Energy : Physical World. (Well, you need energy to run computers obviously, but stick with me here) 
       <p> Having compute grow exponentially has enabled modern AI. Energy can't grow that fast (at least for now), but its growth can enable a new ceiling for us. </p> 
       <p> Fusion to me seems very cool but just not as good commercially compared to fission at the moment. The right approach in my mind is to pursue both. However, the world is resource constrained, so while it's nice to say "pick both" I would say that being aggresive about "is this a feasible idea or is it just a sci-fi pitch" in investments is necessary. </p> 
       <p> Lots of people have made fun of Helion's comment about selling power to Microsoft in 2028 but I'm not laughing. While the timeline might not be fully correct, the mindset from international/slow-moving government projects to silicon valley-esque efforts means that most people's mindsets about how long things take (especially if it's more of a technical than a regulatory issue) are skewed. Very similar vibes to early SpaceX. </p> 
       <p>Go outside and enjoy the weather.</p> 
       <p> - J</p>
     <article> 
       <h2> 15: Calligraphy Doesn't Matter (Clickbait) </h2> 
       <p> Sundays are good days for thinking about the past. </p>  
       <p> During these thinking sessions, regretting past actions is easy. </p>
       <p> However, regret usually doesn't do much good. Instead, I have a better idea for minimizing future instances of these regrets: focus on core ideas. </p>
       <p> Core ideas are why the backgrounds of some fascinating, successful people tend to look disordered or odd to most: the core idea was the same, even if the surface wrapper wasn't. </p>
       <p> For example, I want to learn how to build great things. Whether it's working on rockets or robots or scaling companies, if I focus on the core idea of "what does a great piece of engineering look like," then suddenly the switch flips from "will I regret this" to "this will be useful somehow, even if it isn't immediately obvious." </p>
       <p> This is why the famous Steve Jobs calligraphy example tends to get misinterpreted: Steve always wanted to make great things. In college, that great thing was beautiful typesets. In his later life, those beautiful things were computers. Apple would have been just as innovative in design if Steve had done sculpture, painting, or music composition as he had done calligraphy since it's not the calligraphy that counts but the core idea: how to make a beautiful thing. </p>
       <p>Go outside and enjoy the weather.</p> 
       <p> - J</p>
     <article>
     <article>
      <h2> 14: Aging (Mini Version) </h2> 
      <p> This is going to be the subject of a much, much, much longer blog post but aging is one of those things that more people should be paying attention to. Specifically, the idea that we can change biological age.  </p>
      <p> AI advances over the past year have radically shifted the overton window when it comes to "out there" technologies, and it feels like aging biology startups are where DeepMind and OpenAI were all those years ago. </p> 
      <p> Speaking of DeepMind, did you know that in 2010 even talking about AGI was so radical that Peter Thiel, known for being contrarian, thought that it might be too much for him to invest in Hassabis and Silver to pursue? </p>
      <p> However, bio advances and software advances are not the same, and expecting that bio is "just a software problem" is something to be wary of. We know very little about biology compared to all that it has to offer, but there's signs of cracks coming through. </p>
      <p>Go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
     <article>
      <h2> 13: Bent Flyvbjerg </h2> 
      <p> I discovered Professor Flyvberg last year when I was doing research about how to scale self-driving labs, and his work on megaprojects is phenomenal. </h2> 
      <p> If you haven't already, check him out, and his lessons on building big things is something sorely needed today in Silicon Valley. </h2>
      <p>Go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
    <article>  
      <h2> 12: Something Interesting I've Noticed Pt. 1 </h2> 
      <p>Biology is getting mentioned more and more. </p>
      <p>Paul Graham, Eric Schmidt, Astro Teller, Demis Hassabis, the list goes on. </p>
      <p>One of my friends Elliot Hershberg writes a good blog about biology you should check out: <a href="https://centuryofbio.substack.com"> link here</a> </p>
      <p>Investments from Brian Armstrong and Sam Altman in NewLimit and RetroBio will likely prove prescient.</p> 
      <p>Also ChatGPT releasing an iOS app heals my soul so much. We need to get rid of mediocre "AI" startups, which is what YC and most Berkeley/Stanford people have devolved into.</p>
      <p>Go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
    <article> 
      <h2> 11: TinyStories </h2> 
      <p> How Small Can Language Models Be and Still Speak Coherent English? </p>
      <p> <a href="https://huggingface.co/papers/2305.07759"> HuggingFace </a> </p>
      <p> I think this is currently underappreciated. </p>
      <p>Go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
    <article>
    <article>  
      <h2> 10: Pedal Magic (Not A Scam) </h2> 
      <p>Yesterday I attended a brilliant lecture from <a href="https://twitter.com/adnothing">Adrien Gaidon</a> and there was something fascinating he mentioned after the lecture was over.</p> 
      <p>A big focus of his work is on Embodied Intelligence (which for those who don't have two hours is AI + Robotics) and one of the ways we can learn how to create this embodied intelligence is through studying babies/small children.</p> 
      <p>It turns out that the "best" way to teach your kid how to ride a bike is this <a href="https://www.pedalmagic.com/Default.asp">actually not a scam</a> website. </p> 
      <p>With that knowledge, go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
    <article> 
      <h2> 9: An Announcement and More Thoughts on Robots (Rambling)</h2>
      <p>You can now go to the Everything LLMs And Robotics tab on my website! I put a lot of time and heart into this so I hope that you star, share, and contribute to make it something really special.</p> 
      <p>Now let's get into the rambling.</p>
      <p>Robots are actually starting to work.</p>
      <p>Well, ok, that's complicated but what I mean by that is more general robots are starting to work.</p>
      <p>We've had robots in the form of manufacturing automation for a long, long time, and many things that we don't think of robots now (like dishwashers) have also existed for a long time.</p>
      <p>However, it's not the same as "wait this robot can do lots of things and not just one very narrowly scoped task that we put a million guardrails on"</p>
      <p>I think that LLMs are going to be a big driven for this (by LLMs I mean multimodal models based on the transformer architecture, since just having language as an input is going to seem quaint soon, in the same way that not having a GUI is seen as quaint) since they're just advancing at a breakneck pace.</p>
      <p>I wake up every single day with a migraine and 50 new papers on arXiv I need to sort through, which is good actually. I like it when research moves quickly: more opportunities for me.</p>
      <p>There's arguments about "oh why aren't you working on the pure software side of things why even care about robots since as a field it's always going to be slower than pure software even if it's faster than 99.99999% of other academic areas" and my answer is "even if we got AGI (which again is a vibe stop trying to be so serious about it people go back to category theory if you want to prove something) doing stuff in the real world is hard. Currently doing stuff in the real world is the reason I can eat food, drink water, stay healthy, go places, and build the data centers that advanced AI is going to need anyways. Plus it's so cool everyone loves a robot (this is something Prof Jason Hein remarked during the first ever Accelerate Conference in Toronto, and it hasn't left my mind when showing people videos of stuff.</p>
      <p>(There's a reason Boston dynamics goes so viral.)</p>
      <p>This is also something I've seen when talking to my biomedical friends who are big into drug discovery: besides clinical trials (which vial trials is doing some cool stuff in), our lack of hardcore lab automation is a big reason why it's just so expensive and time consuming to do drug development. (Granted this is an oversimplification and probably won't end eroom's law anytime soon, but Nathan Cheng among others think it's vital.)</p> 
      <p>This is also something that is going on for materials science/chemistry as well, which is also super, super cool.</p>
      <p>I see robots less as replacing human labor and more as a force multiplier in the real world in the same way that GPT-4 is for pure software. (Side note, no wonder openAI is just killing it recently they probably don't have credit limits internally and since they're all incredible unenhanced their ability to ship has got to be crazy) Lights-out factories are very painful because there's always "something" that happens, and having a human around to fix it helps. However, instead of lights out factories I could see small team factories in the same way that small teams of people are now able to build unicorn software-only companies like the original Instagram.</p>
      <p>Oh, also, openAI investing into robots is big. They pulled their original program (rightfully so at the time considering how big the GPT's have gotten) because of a lack of data but now that they're investing back into companies such as 1X I think they've also realized that something is different now.</p>
    <article>
    <article> 
      <h2> 8: I don't have much to say today</h2> 
      <p>Since it's Mother's Day call your mom and tell her you love her.</p>
      <p>Also, since you're still here, I'm going to drop a project I've put a fair bit of effort into on Monday morning (5/15/2023)</p>
      <p>Go outside and enjoy the weather.</p> 
      <p> - J</p>
    <article>
    <article>
      <h2> 7: Questions I think college admissions officers (AOs) should use </h2> 
      <p> Most of the questions AOs ask suck.</p> 
      <p> Exception: Despite being a little bit cringe, at least the University of Chicago's questions are unique.</p> 
      <p> Instead, here's what should be asked: </p> 
      <p> 1) You have 1000 words to explain how to use, clean, declog, and flush a toilet. A reference image is provided on Common App.</p>
      <p> 2) Make or do something of value and explain to me why I should care. 10 words.</p>
      <p> 3) Go outside and enjoy the weather.</p>
      <p>  - J</p>
    <article> 
      <h2> 6: A good form of debate </h2> 
      <p> This was brought up during a conversation with Drew Endy in his car a couple months ago.</p>
      <p> Most modern debate sucks. It sucks because the "goal" is to win, rather than to understand. How could we fix this?</p> 
      <p> Radical empathy, or in less GSB-y terms, "actually listening to each other."</p> 
      <p> Assume a debate has two people, person A and person B. First, let person A speak for 10 minutes about their side with ZERO interruptions.</p> 
      <p> Then, person B can ask person A as many questions as they want until they are satisified. Finally, person B has to restate person A's position until person A is satisfied.</p> 
      <p> Repeat the process where person A is swapped for person B (and vice versa), and this should help to have an actually useful debate.</p> 
      <p> Go outside and enjoy the weather.</p> 
      <p> -J</p>
    <article>
    <article> 
      <h2> 5: Understanding Context Windows </h2> 
      <p>Recently <a href="https://twitter.com/AnthropicAI/status/1656700154190389248"> Anthropic released a version of their Chatbot Claude which has a "100k token" context window.</a> 100k is certainly a big number (and in this context (pun unintended) it actually is) but what does this actually mean?</p>
      <p>For LLMs (Large Language Models) the "context window" refers to the maximum length of text that the model can consider all at once. 
        This is typically measured in tokens, where a token can be as short as one character or as long as one word in English (other 
        languages can have different token lengths due to the nature of the language and the specifics of the tokenization process). 
        For example, in GPT-3, the context window is 2048 tokens. This means that when processing an input, the model looks at the last 
        2048 tokens. If the input is longer than that, it will not be able to consider the earlier parts of the text when generating a response.
        This could lead to it missing important context if the relevant information is outside of this window. 
        The context window is a limitation imposed by the model's transformer architecture, which has a fixed-size attention mechanism. 
        The model cannot pay attention to more tokens than its context window allows. If you want to learn more about transformers,  <a href="https://e2eml.school/transformers.html"> this is a great resource</a> to build your intuition from the ground-up: </p>
      <p>Anyways, now that we have a (basic) understanding of what a context window is, here are some fun examples to help you get a better 
        intuition about what the size of context windows means:</p>
      <p>1 token = 4 character = 0.75 words approximately (taken from OpenAI's Pricing Page)</p>
      <p>2.4k tokens = 1500 words = 4-5 page student essay (Original GPT-3 context window)</p>
      <p>4.96k tokens = 4k words = About a 15 page double spaced essay (GPT-3.5 context window, which is what is currently as of 11 May 2023 used in ChatGPT)</p>
      <p>8k tokens= 6k words = A very low end of a novelette (Current shorter form for GPT-4 as of 11 May 2023)</p>
      <p>32k tokens = 24k words = Respectable length for a novella (Current longer form for GPT-4 as of 11 May 2023)</p>
      <p>100k tokens = 75k words = Great Gatsby or 85 pages of dense tax forms (Anthropic)</p>
      <p>1.2 million tokens = 900k words = Completed Works of Shakespeare (Hypothesized by OpenAI)</p>
      <p>1 billion tokens = 750 million words = Every word ever mentioned about Donald Trump from the announcement of his campaign in June 2015 through August 2015 (Taken from CNN "This is what 750 million words about Donald Trump look like")</p>
      <p>5 billion tokens = 4 billion words = Entire Length of English Wikipedia (As of 8 May 2023)</p>
      <p>Also, fun fact, a transformer (GPT-4) generated the explanation of a context window.</p> 
      <p>Go outside and enjoy the weather.</p>
      <p>- J</p>
    <article>
    <article> 
      <h2> 4: Very Important People </h2> 
      <p>Internalizing lessons is hard. </p>
      <p>Oh sure, it's easy to hear what someone is saying, but to listen? </p>
      <p>To understand? </p> 
      <p>To (as my favorite TA Ahmed once said) feel it in your bones? </p>
      <p>That is difficult. </p>
      <p> "When you grow up you tend to get told the world is the way it is and you're life is just to live your life inside the world. Try not to bash into the walls too much. Try to have a nice family, have fun, save a little money. That's a very limited life. Life can be much broader once you discover one simple fact: Everything around you that you call life was made up by people that were no smarter than you and you can change it, you can influence it, you can build your own things that other people can use. Once you learn that, you'll never be the same again." - Steve Jobs </p>
      <p> You can just do things. </p>
      <p> So do them.</p>
      <p> Go outside and enjoy the weather. </p>
      <p> -J</p>
    <article>
    <article>  
      <h2> 3: Favorite Oscar Wilde Quote </h2> 
      <p> "Some cause happiness wherever they go; others whenever they go." </p> 
      <p> Parallelism ("whe_ever they go" ends both "halves" of the quote) </p> 
      <p> Pause in the middle: Comedic timing in writing (Somehow he can do this) </p> 
      <p> Subversion of expectations: This is going to be happy -> This is not very happy </p>  
      <p> Go outside and enjoy the weather. </p> 
      <p> -J</p>
    <article>
    <article>
      <h2> 2: Carmacking </h2> 
      <p> Carmacking is not a real word but as of today I’m making it one. </p>
      <p> Carmacking, as I call it, refers to legendary programmer John Carmack's current approach to creating AGI, Artificial General Intelligence. For those who don't know, AGI means a lot of things to a lot of people, so it's hard to give an exact definition (and is also why many academics have issues with it, but I choose to accept it as "a vibe rather than a lemma"). Still, it's roughly when a computer is as smart as a human. Again, this is vague, but how Carmack formulates it makes it less so: He thinks it's when a computer can roughly be equal to humans at the vast majority of digital tasks. Due to COVID, so much of the world has shown that it can be moved online, from SWE roles to Zoom meetings, so having digital systems that write business reports seems more likely than an AI superintelligence that destroys humanity. (Well, at least for now. I have more thoughts on that topic I'll write about another time. Maybe if we're stuck in an elevator together someday? (If you know where that is from, I'll pay you 20$, but you have to be right)) </p>
      <p> Anyways, Carmack thinks that the key (or actually keys) to AGI are going to be relatively simple and will seem obvious to us looking back, in the same way, that adding scale to get emergent effects (again, emergence is weird as a definition, but that's another time) seem obvious to us now. This then raises a question: Where are we going to get all of those ideas? </p>
      <p> Carmack's answer is that while we likely haven't discovered every piece, there are some clues in older papers from the 70s-80s starting days of AI to the 90s-early 00s winter-ish periods that might have suffered due to a lack of scale or modern computing. This has merit: The amount of AI research published is impossible for anyone to thoroughly read. When I see AI tools for organizing your AI tools about organizing AI papers, the task of "staying on top of things" feels hopeless. However, I tend to be very skeptical when I'm evaluating an old paper that may have actually been a gold mine if only it had been released in modern times. </p>
      <p> First, the central claim that many people who think that an 80s biophysics or cognitive science article will come in and revolutionize everything is that it could've worked if only there was scale. The typical example they use is artificial neural networks (neural nets), which were fine but didn't become superhuman at many narrow tasks until Toronto researchers made them really deep (lots of layers) and threw a boatload of GPUs at the problem. (Also, length of training time played a factor, but back in 2012, that wasn't nearly as appreciated as it is now.) 
</p> 
      <p> However, today's SOTA (State of the Art) AI systems are massive. They have hundreds of billions of parameters, and to say, "Oh, my algorithm scales well," means something entirely different at this level. Something people forget is that scaling wasn't always assumed to work: it was counterintuitive for most. Many ML/AI researchers back before LLMs and the transformer architecture took off the thought that there would be diminishing returns for scale after a while and that the emergent effects we're seeing wouldn't really exist. Due to the glory of Jensen Huang (and his lovely engineering basement named after him that I currently reside in) and the rise of GPUs for AI research getting better year after year, we've avoided this paradox. (for now)</p>
      <p> Second, I think there's a difference between being inspired by the brain and trying to directly copy what it does. Ideas such as backpropagation and dropout, now crucial to modern AI, are probably not what the brain does. There's more nuance to this argument, where yes, a plane doesn't fly by flapping its wings, but to build an airplane that worked, we had to study birds for centuries. Perhaps the issue isn't the brain itself but rather going too granular. Having systems such as "How can we handle language? Visual stimuli? Sense of itself in physical space? Etc?" could be interesting, but this is more musings from me since I am not a qualified neuroscientist or biologist.</p>
      <p> At the end of the day, this doesn't mean that I think Carmacking is wrong. On the contrary, I think it'll work and that even if it doesn't give us everything, we will find stuff that will be mind-blowing. Fortunately, humans haven't just started thinking about AI when ChatGPT got released, it's been a long ride, and the car hasn't stopped yet. Oh no, we're just starting. </p> 
      <p> Go outside and enjoy the weather. </p> 
      <p> -J </p>
      <article>
      
      
      
    <article>  
      <h2>1.1: A small collection (1) of previous bangers</h2> 
      <p>(To audience of Stanford students) Ok this should be common sense but y'all should actually enjoy your major. I'm a bit obsessive (like I read math papers/write transformers from scratch on Saturdays) but if u wanna die every day don't do it.</p>
      <p>This is your life and don't waste it on stuff that you don't even like. The privilege of a place like Stanford is that you can do (almost) anything. Your limit is your imagination and however much money you can steal from VCs.</p>
      <p>You'll make way more money if you like what you do (at least a little bit) than if you completely hate your job. Why? Exceptional talent is rewarded. A child who's main talent is doing unboxing videos makes more than most CEOs. There are multiple professional whistlers. If you're good you're good.</p>
      <p>Now going for passion as your sole determinant for "this is what I want to do with my life" is flawed (re. The Trouble With Passion (Cich)) but if you have the choice do something you don't absolutely despise. You'll have plenty of time for that when tax season comes around. (unless if you like doing taxes. If that is the case my email is jrin@stanford.edu)</p>
      <p>Go outside and enjoy the weather.</p>
      <p>-J</p>
    </article>
    <article>
      <h2>1: Robots that don't suck</h2>
      <p>Hey everyone, this is my first post on my new little website. I need to still add in stripe stuff and make it not look awful but let’s get down to something I’ve been obsessed with the past few days:</p>
      <p>LLMs are going to make robots not suck for once.</p>
      <p>Explain?</p>
      <p>Robots currently suck. There’s lots of reasons for this, but a big one is that we don’t speak the same language. Humans deal with messy, vague, intuitive language, while
      robots deal with precise, exact code.</p>
<p>A computer will do exactly what you tell it to do.</p>
<p>No more.</p>
<p>No less.</p>
<p>So then, how do we fix this problem?</p>
<p>For the longest time, we didn’t. People slapped a bunch of sensors onto stuff and tried some primitive CV models, but they weren’t sci-fi level.</p>
<p>Now we get to the good part: ChatGPT and GPT-4. Both of these LLMs blew away previously available consumer LLMs, and have rapidly been integrated into a bunch of different robots. While embodied models such as PaLM-E also are leading in the space, nobody outside of Google Research can actually use it, so ordinary people (like me) are going to have to work with OpenAI for now.</p>
<p>Now, the big workflow for how to actually use LLMs for robots is that they essentially just move you up a level of abstraction. Instead of telling the robot through code directly what to do, you tell the LLM to tell the robot through code to do what you want it to do. I haven’t seen anyone try testing code-specific LLMs on these sorts of tasks yet, but I wonder if they would end up being more efficient for this type of work.</p>
<p>However, while this is nice, the most immediate commercial use cases for LLMs in robotics will likely be as a nice UI for databases/robot information. Boston Dynamics was good with this for integrating ChatGPT into their SPOT dog robot and asking it questions such as “what is your battery percentage?” “How many thermal anomalies did you find 30 minutes ago?” “Is that menswear guy on Twitter near my house again please sir stop I’m scared” etc.</p>
<p>Some interesting directions beyond these two are that Microsoft Research used LLMs for drones (my friend Brian also did this for Treehacks, although not at the same scale due to hackathon limitations) and the University of Florida did a study where 15 people tried interacting with a robot through LLMs. (Spoiler alert: It’s actually really good and will probably get deployed in factories once guardrails have been deployed).</p>
<p>Oh yeah. Guardrails.</p>
<p>See, there’s problems with just blindly slapping LLMs into robots. It’s fun when it’s a party trick or helping factories become more efficient, but not when it gets integrated into police or military units. Hallucination is bad but to me prompt injection seems significantly more pressing. Do Anything Now (DAN) and other techniques could cause quite a bit of harm if they aren’t accounted for. Currently I don’t know of any good solutions besides mass RLHF and a very dedicated team of engineers, but if there’s any politicians (or other persons) in NYC that read this blog you should seriously consider this before it gets messy.</p>
<p>Anyways I have things (homework and laundry) to attend to, but this was nice.</p>
<p>Go outside and enjoy the weather.</p>
<p>-J</p>
</article>
  </main>
  <footer>
  </footer>
</body>
</html>
      
      
      
      
      
      
      
      
