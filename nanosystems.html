<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      font-family: 'Times New Roman', Times, serif;
      line-height: 1.6;
      color: #333;
      background: #f8f8f8;
    }
    .container {
      width: 75%; /* Adjusted width to match the painting size */
      margin: 3em auto;
      padding: 2em;
      box-shadow: 0 0 1em rgba(0,0,0,0.05);
    } 
    h1 {
      text-align: center;
      margin-top: 0.5em;
      margin-bottom: 0.5em; 
      font-size: 3 em;
    }
    p, h2, a {
      margin-bottom: 1em;
      text-indent: 1.5em; /* Added text-indent for paragraphs */
    }
    a {
      text-decoration: none;
      color: #0077cc;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      width: 60%; /* Adjusted image to be smaller than the container */
      height: auto;
      display: block;
      margin: 20px auto; /* Increased top and bottom margin for better spacing */
    }
    @media (max-width: 768px) {
      .container, img {
        width: 95%; /* Full width for smaller screens, for both container and image */
      }
      p, h2, a {
        text-indent: 0; /* No indent for paragraphs on smaller screens */
      }
    }
  </style>
</head>
<body>

<div class="container">
  <h1>A Technical Review Of Nanosystems</h1> 
  <h1>By Jacob Rintamaki</h1>
  <img src="images/nanofactory.jpg" alt="Broadway Boogie-Woogie">    
  <h1>Acknowledgements</h1> 
  <p>I’d like to start by thanking Luke Farritor, who has spent many late nights assisting me with this review, Amir Tarkian, who has been extremely knowledgeable on matters of extreme vacuums, and Anton Troynikov, who gave me the idea of shipping something rather than nothing. :)</p>  
  <p>I’d also like to thank My Current [Anonymous] Landlord, Enze Chen, Adam Marblestone, Aydin Gokce, Teddy Ganea, Darson Chen, Martin, Matt Parlmer, Victor Li, Lachlan Sneff, Lukas Suss, Mark Friedenbach, Leo Glikbarg, and Max Newport for their support. </p> 
  <p>If there is anyone I have forgotten, I profusely apologize. I could not have done this/continue to do this alone.</p>
  <h1>tl;dr</h1> 
  <p>The math of Nanosystems seems to work enough to be interesting (even if its not as fast/strong/etc. as the book suggests), yet only a handful of relevant physical experiments have been published.</p>
  <p>I’m going to go do more of these relevant physical experiments and put the results on <a href="https://arxiv.org">arXiv</a>.</p>
  <p>My first experimental goal is to recreate <a href="https://pubmed.ncbi.nlm.nih.gov/12786084/">this paper</a> (where the authors are able to remove and deposit Si atoms on a Si(111)-7x7 surface using an NC-AFM at 78K under UHV conditions).</p>
  <p>I would ideally like to have recreated this experiment by June 20th, 2024 (3 months from the publication of this review, and the summer solstice!), but that's <b>very, very, very, very ambitious.</b></p> 
  <p>However, sometimes <b>very, very, very, very ambitious</b> timelines can enable us to achieve the near-impossible.</p>
  <p>If you think you could help me achieve this <b>very, very, very, very ambitious</b> goal (via advice, connections, equipment, money, etc.) please email me at <a href="mailto:jrin@stanford.edu">jrin@stanford.edu</a> or DM me on Twitter/X at <a href="https://twitter.com/jacobrintamaki">@jacobrintamaki.</a></p>
  <p>Back to work now.</p>

  <h1>Table Of Contents</h1>   
  <h2>Volume 1: The First</h2>
  <h2><a href="#motivation">Motivation</a></h2> 
  <h2><a href="#drama">Drama</a></h2> 
  <h2><a href="#technicalprogress">Technical Progress</a></h2> 
  <h2><a href="#preface">Nanosystems Preface</a></h2>  
  <h2><a href="#ch1">Chapter 1: Introduction and Overview</a></h2> 
  <h2><a href="#ch2">Chapter 2: Classical Magnitudes and Scaling Laws</a></h2> 
  <h2><a href="#ch3">Chapter 3: Potential Energy Surfaces</a></h2> 
  <h2>Volume 2: Back From The Desert</h2> 
  <h2><a href="#ch4">Chapter 4: Molecular Dyanmics</a></h2> 
  <h2><a href="#ch5">Chapter 5: Positional Uncertainty</a></h2> 
  <h2><a href="#ch6">Chapter 6: Transitions, Errors, and Damage</a></h2> 
  <h2><a href="#ch7">Chapter 7: Energy Dissipation</a></h2> 
  <h2><a href="#ch8">Chapter 8: Mechanosynthesis</a></h2> 
  <h2>Volume 3: Coming soon! </h2> 
  <h2><a href="#ch9">Chapter 9: Nanoscale Structural Components </a></h2> 
  <h2><a href="#ch10">Chapter 10: Mobile Interfaces and Moving Parts</a></h2> 
  <h2><a href="#ch11">Chapter 11: Intermediate Subsystems</a></h2> 
  <h2><a href="#ch12">Chapter 12: Nanomechanical Computational Systems</a></h2> 
  <h2>Volume 4: Coming soon! </h2> 
  <h2><a href="#ch13">Chapter 13: Molecular Sorting, Processing, and Assembly</a></h2> 
  <h2><a href="#ch14">Chapter 14: Molecular Manufacturing Systems</a></h2> 
  <h2><a href="#ch15">Chapter 15: Macromolecular Engineering</a></h2> 
  <h2><a href="#ch16">Chapter 16: Paths to Molecular Manufacturing</a></h2> 
  <h2><a href="#appendixa">Appendix A: Methodological Issues in Theoretical Applied Science</a></h2> 
  <h2><a href="#appendixb">Appendix B: Related Research</a></h2> 
  <h2>Transcendent Content</h2>
  <h2><a href="#experimentalplan">Experimental Plan</a></h2> 
  <h2><a href="#updates">Update Log</a></h2> 

  <h1> Volume 1: The First </h1>
  <h1 id="motivation">Motivation</h1>  
  
  
  <p>When you hear the word "nanotechnology," what do you think of?</p>
  <p>To many materials scientists, the word nanotechnology conjures ideas of carbon nanotubes and ever-more-advanced semiconductor fabrication plants trying to cram billions of transistors onto AI chips.</p>
  <p>To some science fiction fans, nanotechnology looks more like a swarm of self-replicating robots that devour everything—a powerful predator.</p>
  <p>But to Eric Drexler, the author of Nanosystems, nanotechnology doesn't look like carbon nanotubes or killer robots but rather a super-fast 3D printer that could make many non-edible physical products atom by atom.</p>
  <p>Drexler calls this vision of nanotechnology APM (atomically precise manufacturing), and if it were invented, it would be one of the most important general-purpose technologies ever.</p>
  <p>Here's an example: if APM worked according to Drexler's calculations, then APM could produce non-edible physical products ten times faster than traditional manufacturing while also having the prices be ten times cheaper. Even better, the product would be of ten times higher quality than its original, typically manufactured version, while also being more sustainable than practically every other manufacturing process we have today. Finally, to make things even more absurd, it's likely that the "ten times" metrics I mentioned earlier are underrating the full potential of APM.</p>
  <p>Let me repeat that again: it's likely that the "ten times" metrics I mentioned earlier are UNDERRATING the full potential of APM.</p>
  <p>That's incredible, but we've only scratched the surface of what APM can do.</p>
  <p>In stories and research papers written by futurists such as <a href="https://bigthink.com/videos/ray-kurzweil-on-the-future-of-nanotechnology/">Ray Kurzweil</a>, <a href="https://mason.gmu.edu/~rhanson/nanoecon.pdf">Robin Hanson</a>, <a href="https://en.wikipedia.org/wiki/The_Diamond_Age">Neal Stephenson</a>, <a href="https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination">Eliezer Yudkowsky</a>, <a href="https://nickbostrom.com/existential/risks.pdf">Nick Bostrom</a>, and others, APM could bring about an age of abundant wealth and scientific knowledge, allowing us to cure diseases, build ultra-powerful computers, and harness the full power of our sun for energy. However, some of these futurists warn of how APM could be utilized (particularly by advanced AI systems) to <a href="https://www.youtube.com/watch?v=Yd0yQ9yxSYY">wipe out</a> all of <a href="https://nickbostrom.com/existential/risks.pdf">humanity</a>, but luckily <a href="http://apm.bplaced.net/w/index.php?title=Molecular_assembler_(disambiguation)">I don't think we have to worry about APM wiping out humanity.</a></p>
  <p>Yet, taking a step back, despite all of this attention from prominent tech futurists, there's surprisingly been very little technical content written about the true feasibility of APM in recent years, and what has been written is mostly of low quality. To make matters worse, there have also been almost no physical experiments with the goal of validating APM published in recent years.</p>
  <p>Now, this doesn’t mean that APM is impossible, since the theoretical foundations for computers and rockets were developed many decades before practical versions of either were built, but there’s still a lot of work left that has to be done to achieve APM.</p>
  <p>So then, let’s get started; we don’t have a moment (or atom) to waste.</p>

  <h1 id="drama">Drama</h1>  
  <p>A common criticism of Nanosystems is that, despite being visionary, there’s been very little experimental progress towards APM since it was published in 1992.</p> 
  <p>(Now, this doesn't inherently mean that the technology is bunk, since both computers (Babbage, Lovelace) and rockets (Tsiolkovsky) had their theoretical foundations somewhat established decades before working versions were built, but it does inspire skepticism in me about the feasibility of APM.)</p>
  <p>The next section of this review covers the last 30 years of technical progress (it’s mixed) towards APM, but there was a large social element that derailed Drexler’s vision.</p>
  <p>This section focuses on that social element, which started in 1986, when Drexler published the popular science book <a href="https://philpapers.org/rec/DREEOC">"Engines of Creation"</a> that he wrote to get the public excited about APM. In this book, Drexler brings up the now-infamous idea of “Grey Goo,” a catastrophe where nanotechnological replicators would exponentially self-replicate, consuming the entire Earth in the process.</p>
  <p>Drexler then discredited the idea immediately after, claiming that, while not impossible, it was inefficient, along with other reasons, but it was too late.</p>
  <p>In the year 2000, Wired magazine published an <a href="https://www.wired.com/2000/04/joy-2/">article</a> by Bill Joy, a cofounder of Sun Microsystems, that warned of the existential dangers of nanotechnology, along with robotics and genetic engineering, because they could self-replicate.</p>
  <p>Robert Freitas, a close collaborator of Drexler, then wrote a paper entitled <a href="https://www.rfreitas.com/Nano/Ecophagy.htm">“Some Limits to Global Ecophagy"</a>, which disavowed the idea of a surprise takeover from Grey Goo. (Freitas argues that these “Grey Goo” incidents are still possible, just that they would be easily detectable and preventable due to the large amount of heat they would generate.)</p>
  <p>However, this paper wasn’t enough, and the public’s fear of Grey Goo led Nobel Chemist Richard Smalley, famous for his discovery of Buckminsterfullerene (also known as "Buckyballs"), to write the article “Of Chemistry, Love, and Nanobots" for the September 2001 edition of Scientific American. In this article, which started the two-year series of back-and-forth correspondences known as the <a href="https://en.wikipedia.org/wiki/Drexler%E2%80%93Smalley_debate_on_molecular_nanotechnology">"Drexler Smalley Debate"</a>, Smalley argued that Drexler’s visions of nanotechnology were flawed for two reasons: sticky fingers and fat fingers. Sticky Fingers refers to Smalley’s claim that atoms will stick to the manipulator’s atom and thus mechanosynthesis is impossible, and Fat Fingers refers to the claim that there isn’t enough room for the 10 or so atoms required to do mechanosynthesis. <a href="http://www.imm.org/publications/sciamdebate2/smalley/">Drexler rebutted</a> the sticky fingers claim by citing the multiple experimental papers that have shown atom-by-atom placement as possible, and the fat fingers claim by saying that mechanosynthesis doesn’t require those 10 atoms in such a small space because that would be impossible, but rather just three or so (example of a CO-functionalized tip on an AFM). Drexler then wrote two additional letters (one in April due to his concerns about Smalley slandering his work and one in June calling Smalley out for not responding to him) to Smalley. Drexler and Smalley’s final debate was in 2003 with a <a href="https://courses.cs.duke.edu/cps296.4/spring08/papers/Drexler.v.Smalley.pdf">“Point-Counterpoint”</a> feature in Chemical & Engineering News (C&EN), where Smalley wrote that Drexler’s vision of grey goo was scaring the children while Drexler advocated for more systems research to develop nanotechnology.</p>
  <p>Although I believe that Drexler won the debate over Smalley, the funding for the newly founded National Nanotechnology Institute (NNI) went almost entirely to Smalley’s vision of nanotechnology, which focused more on chemistry done at small scales than Drexler’s vision of atomically precise nanomachines. Drexler’s 2007 <a href="https://eprints.internano.org/76/1/Productive_Nanosystems_07.pdf">Productive Nanosystems roadmap</a> was also not funded, leaving Drexler’s vision seemingly defeated.</p>
  <p>I would like to note that the loss of scientific momentum in the field of APM was arguably more important than the lack of funding, as APM was derided for many years as pseudoscience. As an example of this derision, Drexler’s 2013 book “Radical Abundance” was reviewed by C&EN (a popular chemistry journal) with the following title: <a href="https://cen.acs.org/articles/91/i30/Delusions-Grandeur.html">“Delusions of Grandeur:</a> [The] author insists ‘atomically precise manufacturing’ will transform civilization, but he’s not dealing with reality.”</p>


  <h1 id="technicalprogress">Technical Progress</h1>  
  <p>There are two main ways that APM could work: either one could have proteins self-assemble in a liquid into enzyme-like molecular machines, or one could have scanning probe microscopes (SPMs) pick up and place down atoms in a cold vacuum environment to make stiff molecular machines. </p>
 
<p>The first approach has been extensively covered by Adam Marblestone in his 85-page bottleneck analysis of <a href="https://web.mit.edu/amarbles/www/docs/Bottleneck_analysis_positional_chemistry.pdf">positional chemistry</a> as well as Richard Jones in his book <a href="https://www.amazon.com/Soft-Machines-Nanotechnology-Richard-Jones/dp/0199226628"> "Soft Machines."</a> The author would like to note that Soft Machines also makes criticisms of APM that are already very rigorously addressed in nanosystems, primarily around concerns of positional uncertainty, the feasibility of mechanosynthesis, and error correction mechanisms, but the rest of the book is an interesting read.</p>
 
<p>I will be covering the second approach for the rest of this review, due to that being the final form of APM. This is not to say that the soft form of APM could not be valuable for bootstrapping up to the SPM form of APM, but it is not my focus and not what I will be running experiments on. </p>
 
<p>Now, this particular section will focus on two questions, with the first being “What are the big technical milestones required for APM?” and the second being, “So then, what work has been done for vacuum and SPM-based mechanosynthesis in APM in the last 30 or so years?” </p>
 
<p>This section will not focus on the question of “so how do we accomplish the remaining big technical milestones required for APM?” because that will be addressed in the next section, Experimentation Plan. </p>
 
<p>Finally, before answering either question, I would like to make a note that one cannot physically buy an SPM, as they are a family of microscopy techniques. Scanning Transmission Microscopy (STM) and Atomic Force Microscopy (AFM) are examples of physical microscopes you can buy, and both are in the SPM family. We will be focusing on AFMs instead of STMs because AFMs work on any type of surface, even if the surface isn’t conducting or semiconducting, but both types of SPM are interesting. (If the reader wants to read up more on how AFMs and STMs work, this 2021 Nature Review of SPMs (https://www.nature.com/articles/s43586-021-00033-2) does a great job.)</p>
 
<p>Now, to answer the first question, I would look at the development of APM in five steps: </p>
<p>1. Mechanosynthesis (of any kind) </p>
<p>2. Making a 3D structure via mechanosynthesis </p>
<p>3. Electrical Motors 
<p>4. Assemblers </p>
<p>5. The First Macroscopic Object </p>
<p>The first step of mechanosynthesis (of any kind; it doesn’t need to be super fancy, just using mechanical means to drive chemical reactions in precise ways) has been justified many times over. Originally, there was Don Eigler's 1990 “IBM” paper (whose actual title was “Positioning single atoms with a scanning tunneling microscope.”). It consisted of spelling out the IBM logo using 35 xenon atoms on a substrate of nickel cooled to 4K (likely liquid helium), but the experimental details were sparse. In the few short years following, these two papers</p>
<p>“Atomic emission from a gold-scanning tunneling microscope tip” (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.65.2418) and “An atomic switch realized with the scanning tunneling microscope” (https://www.nature.com/articles/352600a0) were also written and showed that moving atoms was not a fluke. However, these both relied on STM microscopes rather than AFM. Then, in 2003, a group of Japanese scientists wrote this paper, “Mechanical Vertical Manipulation of Selected Single Atoms by Soft Nanoindentation Using Near Contact Atomic Force Microscopy” (https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.90.176102), which is what I am planning to replicate for my first experiment, where they were able to use a nc-AFM under UHV at 78K to pick up and place down Si atoms on a Si(111)-7x7 surface. This is arguably one of the most important papers in the history of APM, in my opinion, because it showed that Drexler’s version of mechanosynthesis was possible. However, after that, there was a bit of an experimental dark age. The famous “[A] Minimal Toolset for Positional Diamond Mechanosynthesis” (2008) (https://www.molecularassembler.com/Papers/MinToolset.pdf) was released, claiming that diamondoid mechanosynthesis should work, but it was a theoretical study, and no publicly released experiments have directly followed up from this paper. </p>
<p>Around this time, Philip Moriarty (who will be mentioned a lot later) had the idea to run more of these experiments to see if Drexler was right or not, leading to grants from the EPSRC (UK) (https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/G007837/1) to study AFM-based mechanosynthesis. It also led to the Phoenix-Moriarty debate series, which the reader can look up here: Part 1 (http://www.softmachines.org/PDFs/PhoenixMoriartyI.pdf), Part 2 (http://www.softmachines.org/PDFs/PhoenixMoriartyII.pdf), and Part 3 (http://www.softmachines.org/PDFs/PhoenixMoriartyIII.pdf.)</p>
<p>There have been additional proposals to prove the viability of APM (https://www.energy.gov/eere/amo/articles/radical-tool-3d-atomically-precise-sculpting), with one from James K. Gimzewski (who was a co-PI on the Sautet paper we’ll cover later).  but that hasn’t shown any progress. </p>
<p>For a bit of a side-quest around this time, in 2009 Leo Gross (https://research.ibm.com/blog/leo-gross-APS-fellow) createds the qPlus sensor (https://pubmed.ncbi.nlm.nih.gov/19520956/) that can be added onto an AFM, dramatically enhancing the resolution even more. This didn’t directly lead to the seminal IBM “A Boy and His Atom” video (https://www.youtube.com/watch?v=oSCX78-8-q0), but I suspect that it wouldn’t have been possible without high-quality AFM resolution in order to image and then position atoms on a 2D surface. For the IBM “A Boy and His Atom” video (not a paper; they didn’t release anything besides a brief, non-technical video), I’m surprised more folks haven’t done these sorts of things. Ah, well, more for me. </p>
<p>In 2014, there was an additional paper that showcased “Vertical atomic manipulation with dynamic atomic-force microscopy without tip change via a multi-step mechanism" (https://www.nature.com/articles/ncomms5476). The “without tip change” portion is less exciting than it seems and is not able to achieve long-term 3D mechanosynthesis, but it was unusually well documented. The problem with this is that they used an unusual surface, which I don’t think is easily repeatable compared to a more standardized Si(111)-7x7 surface. </p>
<p>Now, for making steps towards 3D mechanosynthesis via AFM, these two papers, “A kilobyte rewritable atomic memory” (https://www.nature.com/articles/nnano.2016.131) and “Assigning the absolute configuration of single aliphatic molecules by visual inspection” (https://www.nature.com/articles/s41467-018-04843-z), are interesting. The latter paper is of particular interest because it actually mentions examining diamondoid structures, which is important because without diamondoid, there isn’t really anything that has the stiffness necessary in order to make molecular machines. </p>
<p>In order to come up to where we are today, there are two additional theory papers of interest, with the first being the 2021 paper from Philip Moriarty entitled “Cyclic Single Atom Vertical Manipulation on a Nonmetallic Surface” (https://pubs.acs.org/doi/full/10.1021/acs.jpclett.1c02271) and the 2023 paper “Controlled Vertical Transfer of Individual Au Atoms Using a Surface-Supported Carbon Radical for Atomically Precise Manufacturing” (https://pubs.acs.org/doi/full/10.1021/prechem.3c00011) by Philippe Sautet’s UCLA group. These two papers take different theoretical approaches to the same idea of vertical manipulation using an AFM in order to build 3D structures, but ultimately, I don’t think either is the right approach. The Moriarty paper relies a lot on crashing into the surface we’re studying rather than creating, say, an ALD layer that would make it easier to retrieve feedstock molecules, and the Sautet paper requires a surface-supported carbon radical to make the energy calculations work, which is not very scalable. </p>
 
<p>That’s basically where we are now, but in order to get to APM, there’s three more conceptual hurdles, I believe, that can serve as good benchmarks besides just 3D structures, which are motors, assemblers, and the first macroscopic object.</p>
 
<p>You could argue motors aren’t a qualitative leap compared to mechano-synthesizing somewhat small 3D structures, but I think because they deal with electromagnetism and are not static objects (motors move!), they deserve to be their own milestone. They are also (Chapter 11) devices that are not solely in their ground state according to transition state theory, so that is qualitatively different from other mechano-synthesized 3D structures. The state of mechano-synthesized motors currently “does not exist,” and the traditional synthesis of molecular motors, while impressive (https://en.wikipedia.org/wiki/Synthetic_molecular_motor), is not anywhere near a Drexler-level motor in terms of performance. </p>
 
<p>Assemblers then become significantly more controversial than motors. The design shown in Nanosystems is 4 million atoms, and even then, it is not well understood. It seems unlikely that this arm, which would only be able to place as fast as an SPM, would be a good choice for the first one, but something smaller like a replicative pixel idea (none of which has been fleshed out due to its speculative nature, so take it with a large grain of salt) should be what works. I see the assembler as the pathway between the microscopic and macroscopic worlds, and hence why it’s the fourth conceptual hurdle needed for APM. </p>
 
<p>However, if assemblers can be built, then the final conceptual hurdle of making a macroscopic object seems within reach. Some might argue that making a macroscopic object when you already have assemblers seems trivial, but even if it follows from reality, there will always be more difficulty in getting experimental ideas to work than expected. This is an area that, despite Drexler working through the calculations in Chapters 13 and 14 of Nanosystems, I will continue to be quite skeptical of until I see it with my own eyes. </p>

  <b>Here’s an additional list of related papers that may be of interest:</b>
  <p><a href="https://pubs.acs.org/doi/10.1021/acsnano.3c10412">Atomically Precise Manufacturing of Silicon Electronics</a></p>
  <p><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=-cuxoSQAAAAJ">Sergei Kalinin’s work</a></p>
  <p><a href="https://pubs.rsc.org/en/content/articlehtml/2014/nr/c3nr06087j">Fab on a chip original paper</a></p>
  <p><a href="https://open.bu.edu/handle/2144/41474?show=full">Fab on a chip PhD thesis</a></p>

  
  <h1 id="preface">Preface</h1>  
  <p><b>Drexler’s Big Idea:</b> The end state of materials science, chemistry, microtechnology, and manufacturing is the precise molecular control of complex structures. In other words, <b>molecular nanotechnology.</b></p> 
  <p><b>Drexler’s Guided by Many Readers:</b> General science background, a student interested in a career in nanotechnology, a physicist, a chemist, a molecular biologist, a materials scientist, a mechanical engineer, or a computer scientist.</p>
  <p><b>Interesting Point:</b> If you look at this from just one perspective, it looks wrong. For example, to pure chemists or pure mechanical engineers, molecular nanotechnology looks wrong, but holistically, it works.</p>
  <p><b>Criticism of Criticism:</b> Some critics see bad nanotechnology designs proposed and then use that to discredit the entire field of molecular nanotechnology, among other things. This is a poor mindset from would-be critics.</p>
  <p><b>Good epistemics:</b> Drexler then has a section saying that “will be” vs. “would be”; he’s not doing false advertising about devices that do or don’t exist.</p>
  <p><b>Thoroughness:</b> Drexler explicitly lays out his approach to citations as “Boltzmann isn’t cited, and neither is novel nanomechanical stuff” (paraphrased), but he tries to cite as much as possible.</p> 
  <p><b>Apologies:</b> Drexler apologizes for being the theorist in an experimentally heavy field, because that’s the hard work that has yet to come.</p> 
  <h1 id="ch1">Chapter 1: Introduction and Overview</h1>  
  <p>Chapter Goal: It’s just an introduction; nothing unusual here.</p> 
<p>A legendary opener: “The following devices and capabilities appear to be both physically possible and practically realizable:</p> 
<p>Programmable positioning of reactive molecules with ~0.1 nm precision.</p>  
<p>Mechanosynthesis at >10^6 operations/device * second </p> 
<p>Mechanosynthetic assembly of 1 kg of objects in <10^4 s</p> 
<p>Nanomechanical systems operating at ~10^9 Hz </p> 
<p>Logic gates that occupy ~10^-26 m^3 (~10^-8 mu^3) </p> 
<p>Logic gates that switch in ~0.1 ns and dissipate <10^-21 J </p> 
<p>Computers that perform 10^16 instructions per second per watt </p> 
<p>Cooling of cubic-centimeter, ~10^5 W systems at 300K </p> 
<p>Compact 10^15 MIPS parallel computing systems </p> 
<p>Mechanochemical power conversion at >10^9 W/m^3 </p> 
<p>Electromechanical power conversion at >10^15 W/m^3 </p> 
<p>Macroscopic components with tensile strengths >5 x 10^10 GPa </p> 
<p>Production systems that can double capital stocks in <10^4 s”</p> 
<p>Here’s the context and math to double-check, which will involve the use of future chapters.</p> 
<p>Programmable positioning of reactive molecules with ~0.1 nm precision. </p> 
<p>Given that mechanosynthesis involves moving things atom by atom, this is essentially a rephrasing of that. This has already been demonstrated in the technical progress portion of this review. </p> 
<p>Mechanosynthesis at >10^6 operations/device * second </p> 
<p>This is based on being in a vacuum, because while 10^6 operations/device * second sounds fast at first, if this is done atom by atom, then this would imply an assembly frequency of 1 MHz. Now, resonant frequencies for atomic force microscopes can be in the hundreds of megahertz (https://www.nature.com/articles/s41378-022-00364-4), and thus if every 100 out of the many vibrations per second that are occurring end up being able to pick up, place down, and then adjusting (imaging at the macroscale could theoretically be done with a laser interferometer instead of a CO-functionalized tip once feedback-force curves have been characterized enough, allowing for more breathing room, and I would also like to note that this is a rough approximation), it could be reasonable to do 10^6 operations/device * second. This would be more constrained for mechanosynthesis-designed AFMs at the nanoscale, but it’s not unreasonable. </p> 
<p>Mechanosynthetic assembly of 1 kg of objects in <10^4 s </p> 
<p>10^4 seconds = 10,000 seconds, or roughly 2.77 hours. In 14.7, Drexler’s calculations for convergent assembly add up to saying a 1 kg system can build a 1 kg product object in roughly 1 hour, dissipating 1.1 kW of heat. This argument primarily relies on an idea called convergent assembly rather than having one machine place down every single atom (even at rates of 10^6 operations per device * second, that would take 10^18 seconds for a device with 10^24 atoms, which is quite long!). This is one area where I’m actually skeptical about calculations given that we are not anywhere close to convergent assembly, so I’m leaving it in a state of limbo. Much of this argument rests on the idea that we’ve thought of the majority of frictional modes and other such factors that somewhat hinder nanomechanical computing in order to exponentially assemble macroscopic objects. </p> 
<p>Nanomechanical systems operating at ~10^9 Hz </p> 
<p>In chapter 14 (specifically 14.5.3), Drexler mentions devices operating at around 10^6 Hz and doesn’t seem to mention 10^9 Hz very frequently (although Chapter 10.10.2c mentions diamondoid nano-mechanisms operating for years at >10^9 Hz). However, if we take into account a fast AFM, then this figure doesn’t seem completely unreasonable (although more around 10^8 Hz), but I would rather stick to around 10^6 Hz as that’s what Chapter 14 uses for its convergent assembly math, and that’s less extreme than a billion ticks per second (1 GHz). </p> 
<p>Logic gates that occupy ~10^-26 m^3 (~10^-8 mu^3) </p> 
<p>Right now, as of 2024, we’re around the 3 nm “node” (semiconductor industry terminology that refers to a level of technology), which has about a 48 nm gate pitch and a 24 nm metal pitch. Roughly modeling a logic gate as a cube with 24 nm sides, that would be a volume of around 1.3824*10^-23 m^3. There’s still room to go with current nodes, so the volume of a logic gate could be ever smaller, but this would represent 1-2 orders of magnitude smaller logic gates. This claim isn’t as wild as Drexler’s other claims, but perhaps it’s wild that Moore’s Law has continued for so long. (FinFET, a pivotal technology, wasn’t even invented until 7 years after Nanosystems!). </p> 
<p>Logic gates that switch in ~0.1 ns and dissipate <10^-21 J </p> 
<p>Chapter 12 seems to be signal speed propagation of diamond/10 is like 1.7 km/s, and then scaling that down to the 1 um scale is 0.6 seconds. In Chapter 12.3.4d, it’s 0.05 maJ (milliatto-joule, or 10^-21 joules), which means that yes, it dissipates under 10^-21 joules. However, comparing this to modern logic gates, modern logic gates are usually on picosecond (10^-12 second) switching times, so rod logic computers don’t perform as well there, but modern computers are usually 10^-18 joules for switching, which means that rod logic could potentially have an advantage for energy efficiency. Personally, I suspect the main advantage of rod logic computing (Chapter 12) is physical compactness rather than speed or power efficiency, for which nanoelectronics could do quite well. Drexler is just wary of nanoelectronics because they do exhibit quantum effects, which make them non-trivial to model. </p> 
<p>Computers that perform 10^16 instructions per second per watt </p> 
<p>Chapter 14.12.9 conclusions mentions this, but I don’t see the explicit calculations being carried out anywhere in Nanosystems. Also, this is challenging because ISP/W doesn’t get used a ton nowadays as compared to FLOPs (floating point operations per second) due to the deep learning and GPU revolutions (Nvidia did not exist in 1992!). This would be many orders of magnitude more than current computers, even GPUs, however, and could mainly be attributed to energy efficiency rather than raw computational power, if I had to make a guess. I would leave the verdict on this one as “plausible, but very vague and needs a lot more math.” </p> 
<p>Cooling of cubic-centimeter, ~10^5 W systems at 300K </p> 
<p>In Chapter 11.5, Drexler directly addresses the cooling of cubic-centimeter, 10^5 W systems at 300K. First, it’s 273K (the melting point of ice) rather than 300K, and there are two interesting points being made. The first is that the design of distinct heat-carrying bodies parallels the use of red blood cells as oxygen carriers in the circulatory system. This is what the best of Drexler looks like to me: looking to biology for inspiration and then using deterministic manufacturing in order to build better systems for various purposes. The second interesting point is that from Heimenz’s 1986 “Principles of Colloid and Surface Chemistry,” an expression derived from the Einstein relationship relates the viscosity of a fluid suspension n, the volume fraction of spherical particles fvol, and the viscosity without the particles n0, as n/no = (1-fvol)^-2.5, and is reasonably accurate for fvol <=0.4. Packaged ice particles with a flow rate of 0.4 increase the fluid viscosity by a factor of 3.6, and if pentane is used as a carrier, the viscosity of the resulting suspensions is 8*10^-4 N*s/m^2. On melting, ice particles at this volume fraction absorb 1.2*10^8 J/m^3 based on coolant volume. Plugging this into the equation, this factor of energy dissipation makes sense, both mathematically and analogously. </p> 
<p>Compact 10^15 MIPS parallel computing systems </p> 
<p>MIPS is a family of RISC ISAs (and MIPS has announced in 2021 that it’s transitioning to the RISC-V ISA). Right now it’s Chapter 12 that mentions this in Nanosystems, and the conversion from MIPS (Million Instructions Per Second) to a more modern measure of computing power, FLOPs (Floating Point Operations Per Second), is around a million FLOPs (10^6) in one MIPS, but I would like to explicitly note that this is likely wrong for many reasons. One of the reasons this estimate would be wrong is that floating point operations can take more clock cycles to complete than integer operations, but also that many processing systems can conduct multiple floating point operations during their clock cycles due to various performance tricks like SIMD (single instruction, multiple data) or more. This is not my area of expertise, but if we did take that million FLOPS to one MIP comparison, then this would be 10^21 FLOPs in a compact zone, while Nvidia’s recent Blackwell B200 GPU is quite large and would deliver 20 exaflops (around 2*10^16 FLOPs per unit). (https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100) This is an area that would be a major use case for nanomechanical computing, as that kind of computing power in such a small space would be incredible, whether for medical nanorobots or other applications. </p> 
<p>Mechanochemical power conversion at >10^9 W/m^3 </p> 
<p>Chapter 14.5.5 points to Chapter 13.3.8. I immediately see problems with this, namely that it involves the reaction 2H2 + O2 -> 2H2O, which is normally explosive (it’s the reaction of LH2 and LOx that rockets—not all, because many use methane instead of LH2 these days, in part because you can create methane in situ electrochemically on Mars). Section 8.5.10: Transition-metal reactions is said to provide justification for this, but I’m not sure I see the logic here. Besides, mechanochemical power would not be worthwhile compared to electromechanical power conversion, so I’m not that mad at this being quite off. </p> 
<p>Electromechanical power conversion at >10^15 W/m^3 </p> 
<p>Chapter 11.7.3: Motor power and power density covers this section, and I am immediately skeptical for many reasons: 1) Drexler makes the assumption of “neglecting resistive and frictional losses,"  which is not something I would immediately do, and the power density is so large because the volume of the motor is incredible small and the stiffness of the diamondoid structures can support high centrifugal loads. I would shelve this under “not believable” IF viewed at the macroscale, because at the microscale these incredible densities would hold, but the entire amount of solar radiation that the earth intercepts is 10^17 watts, which would imply about 1000m^3 of these motors would be more than the entire sun hitting the Earth, which is absurd. </p> 
<p>Macroscopic components with tensile strengths >5 x 10^10 GPa </p> 
<p>I don’t like to leave a section like this because I’m assuming there’s something I clearly must be missing, but I haven’t seen this example specifically mentioned anywhere in Nanosystems. Off the top of my head, though, I would suspect that this is wrong (although likely much higher than what exists today) because it relies on continuum models at the nanoscale, which are then convergently assembled into macroscopic objects, thus making some assumptions that we shouldn’t. The strongest argument I could see for high tensile strengths would be that diamondoid is incredibly stiff, and if it (like carbon nanotubes, both carbon, by the way) were grown in long, macroscopic, thin tubes, then it would have an incredible tensile strength. </p> 
<p>Production systems that can double capital stocks in <10^4 s </p> 
<p>In chapter 14, for the conclusion, Drexler mentions that a 1kg system can build a 1kg product object in about 1 hour, dissipating about 1.1 kW of heat. This would be less than the roughly two and a half hours that 10^4 seconds would imply, and is perhaps the most astonishing thing besides the compact 10^15 MIPS computer that is on this first page of Nanosystems. The mathematics for how this is feasible is written in Chapter 14.4, and it doesn’t seem too unusual with the exception that even if the components operated at their stated frequencies (and then Drexler cuts it by three for a margin of error), that’s still potentially quite efficient. The problem with nanomechanical computing is that as it got studied, more frictional modes were discovered, reducing its efficiency. If this were not possible (doubling capital stocks in roughly an hour), it would likely be because the compounded efficiencies of the motors, gearboxes, sensors, etc. are much worse than Drexler makes them out to be. </p> 
<p>Definition Time! (Because this will be an issue 10 years after Nanosystems is written for Drexler-Smalley and other debates.) </p> 
<p>Molecular Manufacturing: “The construction of objects to complex, atomic specifications using sequences of chemical reactions directed by non-biological molecular machinery.”</p> 
<p>Molecular Nanotechnology: “...describes the field as a whole.”</p> 
<p>Mechanosynthesis: “Mechanically guided chemical synthesis” </p> 
<p>Machine-Phase Systems: A system in which all atoms follow controlled trajectories (within a range determined in part by thermal excitation). Machine-phase chemistry describes the chemical behavior of machine-phase systems, in which all potentially reactive moieties (a portion of a molecular structure having some property of interest) follow controlled trajectories. </p> 
<p>Important: Don’t confuse these with old fields; they’re new fields. </p> 
<p>CONTEXT: In 1992, positional chemical synthesis was at the threshold of realization. Eigler and Schweizer's 1990 paper, “Positioning single atoms with a scanning tunneling microscope,” (https://www.nature.com/articles/344524a0). However, advanced positional chemical synthesis is still in the realm of design and theory.</p> 
<p>Table 1.1: Some known issues, problems, and constraints </p> 
<p>Thermal excitation </p> 
<p>Chapter 5</p> 
<p>Thermal and quantum positional uncertainty </p> 
<p>Chapter 5</p> 
<p>Quantum-mechanical tunneling </p> 
<p>Chapter 5</p> 
<p>Bond energies, strengths, and stiffness </p> 
<p>Chapter 3</p> 
<p>Feasible chemical transformations </p> 
<p>Chapter 8, but this is scattered around a lot more. I’d still go primarily with Chapter 8 because that’s the mechanosynthesis chapter. </p> 
<p>Electric field effects </p> 
<p>Chapter 11</p> 
<p>Contact electrifications </p> 
<p>Chapter 11</p> 
<p>Ionizing radiation damage </p> 
<p>Chapter 6</p> 
<p>Photochemical damage </p> 
<p>Chapter 6 </p> 
<p>Thermomechanical damage </p> 
<p>Chapter 6</p> 
<p>Stray reactive molecules </p> 
<p>Chapter 6</p> 
<p>Device operational reliabilities </p> 
<p>Chapters 10 and 11 primarily state that a device doesn’t mean a macroscale system. </p> 
<p>Device operational lifetimes </p> 
<p>Chapters 10 and 11 primarily state that a device doesn’t mean a macroscale system.</p> 
<p>Energy dissipation mechanisms </p> 
<p>Chapter 7 </p> 
<p>Inaccuracies in molecular mechanics models </p> 
<p>Chapter 3</p> 
<p>Limited scope of molecular mechanics models </p> 
<p>Chapter 3</p> 
<p>Limited scale of accurate quantal calculations </p> 
<p>Chapter 3</p> 
<p>Inaccuracy of semiempirical models </p> 
<p>Chapter 3 </p> 
<p>Providing ample safety margins for modeling errors </p> 
<p>Does this throughout.</p> 
 
<p>Drexler seems to be fine with pointing out errors, with one exception: he assumes that failures happen fast and are easily detectable rather than being slower and more insidious. That was the one criticism Eliezer Yudkowksy made about Nanosystems (https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination), and it does look like a genuine blindspot. Drexler makes this “single point of failure” assumption because, on the atomic and molecular scales, if something goes wrong, it will cause operation failure quite quickly due to the small distances and high frequencies of atomic vibrations. However, as things scale, this doesn’t hold, and while Drexler eventually will mention radiation damage as breaking this “single point of failure” assumption, I don’t think he talks about it enough. To deal with this, we could have solutions for redundancy and monitoring and testing devices over long periods of time. Drexler already builds a lot of redundancy into his designs, so that’s fine. Adding internal sensors and additional forms of monitoring (internal and external) could be useful, but that feels like a further concern at the moment. </p> 
<p>Table 1.2: Contrasts between solution-phase and mechanosynthetic chemistry (Table 8.1 in Chapter 8 is more detailed) </p> 
<p>Comparisons: </p> 
<p>Conventional fabrication and mechanical engineering </p> 
<p>Similarities:Both have components, systems, controlled motion, and rely on traditional ideas of manufacturing rather than synthesis. </p> 
<p>Differences: APM will be much smaller and have to deal with molecular phenomena. </p> 
<p>Verdict: Agree with Drexler. At a high level, there are many similarities that we can borrow, but at a low level, these intuitions must be abandoned. </p> 
<p>Microfabrication and microtechnology </p> 
<p>Similarities: Both are on small scales and have quantum effects. </p> 
<p>Differences: Drexler claims that technologies like photolithographic pattern definition, etching, deposition, diffusion, etc. </p> 
<p>Verdict: I disagree with Drexler; I think that microfabrication and microtechnology will be significantly more important than he expects. However, this is because in the 30 years since Nanosystems was released, these technologies have rapidly improved and are nearing the atomic scale, although there are major issues and tradeoffs for precision, throughput, reliability, etc. </p> 
<p>Solution-Phase chemistry </p> 
<p>Similarities: Both rely on molecular structure, processes, and fabrication.</p> 
<p>Differences: Drexler focuses on mechanosynthesis and machine-phase systems. </p> 
<p>Verdict: Complete opposites, but some principles of solution-phase chemistry could be useful here. </p> 
<p>Biochemistry and molecular biology </p> 
<p>Similarities: Both have molecular machines (ribosomes in particular are a great model system and are used frequently in Drexler’s rebuttal arguments) and molecular systems. </p> 
<p>Differences: biology uses polymeric materials instead of diamondoid materials and works with stochastic, solution-based chemistry rather than the precise machine-phase chemistry Drexler proposes. </p> 
<p>Verdict: Ribosomes are an interesting model system, but I don’t think biology is a good proof of existence for machine-phase chemistry since the materials and environment are radically different. </p> 
<p>From Drexler: While full quantum electrodynamic (QED) simulations of nanosystems would be ideal, we must make assumptions for computational tractability because that’s how engineering works. </p> 
<p>Scope and assumptions: </p> 
<p>A narrow range of structures </p> 
<p>Author: Very fair. </p> 
<p>No nanoelectronic devices </p> 
<p>Author: Personally, I dislike this choice a lot.</p> 
<p>Machine-phase chemistry </p> 
<p>Author: Fair. </p> 
<p>Room temperature processes </p> 
<p>Author: Enjoy this. </p> 
<p>No photochemistry </p> 
<p>Author: Good choice can be fickle. </p> 
<p>The single-point failure assumption </p> 
<p>Author: Controversial, I addressed this previously. </p> 
<p>Objectives and non-objectives </p> 
<p>Not describing new principles and natural phenomena </p> 
<p>Author: Good choice.</p> 
<p>Seldom formulating exact physical models </p> 
<p>Author: To be fair, computational tractability is an issue.</p> 
<p>Seldom describe immediate objectives. </p> 
<p>Author: I disagree with this choice, particularly considering how much of a deviation from the traditional chemistry paradigm this is. </p> 
<p>Not portraying specific future developments </p> 
<p>Author: Fair.</p> 
<p>Seldom seek an optimal design in the conventional sense. </p> 
<p>Author: Maximizing efficiency is of secondary concern for the purpose of this book over derisking additional areas. Good choice.</p> 
<p>Seldom specifying complete detail in complex systems </p> 
<p>Author: The author understands why Drexler made this choice, but the author disagrees with it and believes it hurts the perception of nanosystems in the scientific community.</p> 
<p>Favoring false-negative over false-positive errors in analysis </p> 
<p>Author: Good choice.</p> 
<p>Describing technological systems of novel kinds and capabilities </p> 
<p>Author: Ok.</p> 

  <h1 id="ch2">Chapter 2: Classical Magnitudes and Scaling Laws</h1>  
  <p>Chapter Goal: “When used with caution, classical continuum models of nanoscale systems can be of substantial value in design and analysis.” (DISAGREE!)</p> 
<p>Approximation is inescapable. Why? The most accurate physical models (first-principles quantum mechanics) are computationally intractable. This gets discussed a lot in the next chapter. </p> 
 
<p>Basic assumptions for mechanical systems are: constant stress, which implies scale-independent elastic deformation; scale-independent shape; scale-independent speeds; and constancy of space-time shapes describing the trajectories of moving parts. </p> 
<p>Author’s Note: I do not agree with this assumption because size-determining unusual properties is a central idea of modern nanoscience (which is separate from APM).</p> 
<p>The choice of diamond as a material is interesting: density (3.5 * 10^3 kg/m^3), Young’s modulus (10^12 N/m^2), low working stress (10^10 N/m^2).</p> 
<p>What do these assumptions mean? “Larger parameter values (for speed, acceleration) relative to those characteristic of more familiar engineering methods.” Things move fast at the nanoscale. </p> 
<p>Is that intuition good? Yes. Biology is quite fast at the nanoscale (an analogy we’ll use and then not use in various sections of this review). Look at the website “Cell Biology by the Numbers” (http://book.bionumbers.org/) (a Laura Deming favorite) to learn more about this. </p> 
<p>Another big idea of this chapter is that “scaling principles indicate that mechanical components can operate at high frequencies, accelerations, and power densities.” </p> 
<p>Is this idea right? Yes, for the same reason the biology analogy I brought up in this chapter reviews earlier works. However, how high “high frequencies, accelerations, and power densities” are could be up for debate, since 10^6 Hz and 10^9 Hz are both high frequencies but are quite different things in practice. </p> 
<p>Adverse scaling of wear suggests that bearings are a special concern. This is a good point that Drexler brings up, since wear and friction are two areas that commonly get brought up as complaints against nanosystems being possible. </p> 
<p>Author’s Note: This is around the moment where I begin to severely disagree with Drexler, since for the next few sections he covers classical mechanical, electromagnetic, and thermal systems (where all of the derivations he makes from classical continuum systems are correct, or at least have their dimensional analysis done right, for which I will not bore you).  However, although Drexler does acknowledge that high-frequency quantum electronic devices will be extremely dissimilar to classical continuum models, I don’t think he mentions how much continuum models break down at the nanoscale. For example, he does mention that our macroscopic understanding of heat breaks down on the atomic level and that it works more like energy being transferred ballistically by phonons, for which the mean free path, in the absence of bounding surfaces, would exceed the dimensions of the structure in question. (This is also a concern when designing UHV chambers, because the mean free path for the handful of gas molecules inside can be many kilometers long or even more.) but then the conclusions feel uncertain to me. In particular, there’s a Table (2.1) that Drexler includes in this chapter, and I really dislike it because one of its main metrics is whether a physical quantity has a “good, moderate to poor, moderate to inapplicable, bad, good at small scales, or definitional” rating, and he doesn’t explain what those mean or how he assigned them beyond what appears to be "vibes.”.</p> 
<p>Further chapters consider more limits and edge cases. In particular, AC electrical circuits are different from these models. (They would be different because the failure modes for complex electromagnetic systems can be astounding: short circuiting, overloads, arc faults, insulation breakdown (which will be an issue when in Chapter 11 we discuss motor designs), harmonic distortion, voltage spikes or surges, etc.) </p> 
<p>Author’s Note: This is good, but I would almost say to scrap any continuum model used at the nanoscale, even if it dramatically increases the computational cost of our models. </p> 
<p>Conclusions: Scaling laws are bad for electromagnetic systems with small calculated time constants, reasonably good for thermal systems and slow-varying electromagnetic systems, and often excellent for mechanical systems as long as the component dimensions substantially exceed atomic dimensions. Scaling principles indicate that mechanical components can operate at high frequencies, accelerations, and power densities. The adverse scaling of wear lifetimes suggests that bearings are of special concern.</p> 

  <h1 id="ch3">Chapter 3: Potential Energy Surfaces</h1>   
  <p>Chapter Goal: This chapter talks about potential energy surfaces, which are fundamental to practical models of molecular structures and dynamics. </p> 
<p>Author’s Note: This chapter (and chapters 4–7, which cover various molecular dynamics methods and their implications) have made arguments of AI doom that rely on an AI system that’s able to “magically solve these problems computationally” seem significantly weaker to me. It’s not that I don’t think AI models for molecular systems could get significantly better (they could and they will), but for how many parts a macroscale nanodevice is made out of, this will require some serious experimentation in the loop and feedback testing. In the George Hotz vs. Eliezer Yudkowsky Youtube Debate (https://www.youtube.com/watch?v=6yQEA18C-XI), Eliezer takes the position that this is somewhat like a special case of the protein folding problem (bootstrapping APM) and makes an interesting point that it could maybe rely on the past corpus of experiments. However, due to how many approximations we have to make for our calculations, there is a high level of empiricism required for developing APM (computations are for invalidation rather than validation, and drug/material screening looks a lot like “a weak computation screens out 99%, the remaining 1% goes through a more rigorous computation that screens out 99% of that 1%, and then the remaining 0.01% has experiments run on it to find promising compounds) that Hotz seems to correctly understand. </p> 
<p>DEFINITION: Molecular potential energy surfaces, which “...stem from a visualization in which a potential energy function in N dimensions (that is, in configuration space) is described as a surface in N + 1 dimensions, with energy corresponding to height. (When N>2, the visualization is necessarily nonvisual.)”</p> 
<p>Visualization: significant features of a PES are its potential wells and the passes (cols) between them. “A stiff, stable structure resides in a well with steep walls and no low, accessible cols leading to alternative wells. A point representing the initial state of a chemically reactive structure, in contrast, resides in a well linked to another well by a col of accessible height. A point representing a mobile nanomechanical component commonly moves in a well with a long, level floor.” </p> 
<img src="images/Potential_Energy_Surface_for_Water.png" alt="water PES">
<p>(Potential Energy Surface for Water, https://en.wikipedia.org/wiki/Potential_energy_surface#/media/File:Potential_Energy_Surface_for_Water.png)</p> 
<p>HUGELY UNDERRATED POINT: Drexler makes an analogy saying that “molecular systems in which all transitions occur between distinct potential wells resemble transistor systems in which all transitions occur between distinct logic states: in both instances, the application of correct design principles can yield reliable behavior even though other systems subject to the same physical principles behave erratically.” </p> 
<p>I believe this analogy is a mostly valid one to make, but it is quite load-bearing: if we weren’t able to rely on these distinct transitions that were analogous to logic states, then all of this would fall apart. </p> 
<p>BIG CONCEPT: time vs. accuracy trade-off for simulations. You can have fast simulations or accurate simulations, but not fast, accurate simulations. </p> 
<p>Drexler doesn’t directly use wave equations or the mathematics of quantum mechanics here; that’s too low a level for the work we want to do. The goal is to think more like a chemist or materials scientist than a particle physicist. </p> 
<p>Quantum electrodynamics (QED) is a very accurate theory of electromagnetic fields and electrons that would be lovely to use if it weren’t so intractable for relevant systems. </p> 
<p>Schrodinger’s equation is also intractable for relevant systems. </p> 
<p>Why? Imagine you want to model a 1-D differential equation. If you put down 10 points to approximate it, your approximation will be pretty good. If you want to model a single atom in 3D space, you’ll need 10 points for each of the cartesian X, Y, and Z dimensions to model it, so it’ll be 10^3 points to consider. Now to model 300 atoms, you’ll need 10^(3 dimensions * 300 atoms) = 10^900 points to model the system, which is too much for any computer we have.</p> 
<p>Relativistic effects should be considered, but for valence electrons (especially for lighter elements), they're chemically negligible. Mostly. In 8.4.4b, Drexler discusses a case where that might not be the case, where relativistic effects in heavy atoms can cause strong spin-orbit coupling. </p> 
<p>Relevant: For chemistry and nanomechanical engineering, the full wave function gives more information than is necessary. </p> 
<p>Born-Oppenheimer Approximation: Treats the motion of electrons and nuclei separately. Nuclei are around 2,000 times bigger than electrons. If momentum = mass * velocity, to get the same velocity as a similar-moving electron, a nucleus would need 2,000 times the momentum. Thus, we can treat the nuclei as fixed compared to the electrons. </p> 
<p>Where does the Born-Oppenheimer approximation break down? When nuclear motions are fast (nuclear bombs), when excited states occur at very low energies (rare in stable molecules), and when small changes in nuclear coordinates cause large changes in the electron wave function (also rare). </p> 
<p>Potentially controversial statement: “In most nanomechanical systems, as in most chemical reactions, electron wave functions change smoothly with changes in molecular geometry.”</p> 
<p>Verdict: Not very controversial, but be careful for (literal) edge cases. </p> 
<p>Practical calculations require further approximations. </p> 
<p>The most popular approaches are known as molecular orbital methods. </p> 
<p>Independent-electron approximation: Treats each electron in a molecule as moving in an averaged field created by the other electrons, simplifying calculations.</p> 
<p>What’s the catch? This approximation neglects electron correlation, where the movement of one electron is influenced by the instantaneous position of others, potentially leading to significant errors in the description of electron behavior, especially in systems where electron-electron interactions are strong.</p> 
<p>Approximate wave functions:Approximate wave functions are used to solve the Schrödinger equation when exact solutions are not possible, using methods like Hartree-Fock to estimate electron distributions.</p> 
<p>What’s the catch? These approximations can fail to account for all electron correlations and might not accurately represent the true nature of the electron density, especially in excited states or systems with close-lying energy levels.</p> 
<p>Configuration Interaction (CI): improves upon the Hartree-Fock approximation by considering a superposition of multiple electron configurations, accounting for electron correlation more accurately.</p> 
<p>What’s the catch? CI can become computationally very demanding with increasing system size because the number of possible electron configurations grows exponentially, often making it impractical for large systems.</p> 
<p>Semiempirical methods (ab initio): use approximations and empirical parameters to simplify calculations, trading some accuracy for computational efficiency.</p> 
<p>What’s the catch? The empirical parameters may not transfer well between different systems or states, and the approximations can limit the method's predictive power, making these methods less reliable for systems outside the range for which the parameters were fitted.</p> 
<p>However, due to concerns about the expense, accuracy, and scalability of these methods, we can also look into molecular mechanics methods, which use molecular geometry instead of electronic structures for energy calculations. </p> 
<p>Fun fact: organic chemists represent molecular structures with ball-and-stick models: each ball is an atom, and each stick is a bond. </p> 
<p>For small molecules, the computational cost favors molecular mechanics over ab initio methods by a factor of 10^3. For larger systems, it can be even larger because molecular mechanics methods scale by the second or third power of the number of atoms (O(N^2) or O(N^3), ignoring some constant factor) rather than ab initio methods, which may be in the fourth or higher regimes (O(N^4)). </p> 
<p>A limitation of molecular mechanics is that it works best when the systems are not too far from equilibrium. I don’t think it will be much of an issue for, say, a solid block of atoms (that’s essentially just in one energetic state), but for modeling motors or moving objects, then molecular mechanics would likely not be a good choice. </p> 
<p>Thus, computations that aren’t essentially QED should be used more for invalidating certain designs than validating them. A simulation could tell you, “This won’t work,"  but only an experiment will tell you, “This will.”</p> 
<p>The MM2 model The current most-used force fields are either MM4 or are learned through machine learning models trained on quantum-chemical DFT data. (I'll show how MM2 and MM4 are different here.)</p> 
<p>Bond stretching: </p> 
<p>MM2 uses a simple harmonic oscillator model for bond stretching, considering the bond lengths and their deviations from equilibrium values.</p> 
<p>MM4 introduces anharmonic terms to better represent the potential energy associated with bond stretching, especially at larger deviations from the equilibrium bond length.</p> 
<p>Bond angle bending</p> 
<p>MM2 employs a quadratic function for angle bending, treating deviations from the equilibrium bond angle as a simple harmonic motion.</p> 
<p>MM4 incorporates more sophisticated functions, including anharmonic terms, to account for the energy changes associated with bond angle bending more accurately, especially for large deviations.</p> 
<p>Bond Torsion</p> 
<p>MM2: Models torsional interactions using a Fourier series or cosine functions to represent the energy variations with rotational angles of bond torsions.</p> 
<p>MM4 enhances the description of torsional potentials, adding terms to better capture the complexities of molecular conformational changes and stereoelectronic effects.</p> 
<p>Electrostatic Interactions</p> 
<p>MM2 calculates electrostatic interactions using Coulomb’s law, often with fixed partial charges on atoms.</p> 
<p>MM4: May include more advanced treatments of electrostatics, considering charge transfer, polarization, and possibly an improved charge distribution model.</p> 
<p>Nonbonded Interactions</p> 
<p>MM2: Accounts for van der Waals forces using Lennard-Jones potential or similar functions.</p> 
<p>MM4 includes explicit treatment of hydrogen bonding and improved van der Waals parameters.</p> 
<p>Complications and conjugated systems</p> 
<p>MM2: It doesn’t really capture some of the unusual subtleties of conjugated and aromatic systems or the effects of electron delocalization.</p> 
<p>MM4: Improved handling of conjugated systems, electron delocalization, and aromaticity, providing better energy and geometry predictions for complex organic molecules.</p> 
<p>So, overall, does the MM2 to MM4 switch change anything? It's hard to say, but I don’t think so. These simulations are still tools for disproving designs rather than proving designs. We need to run more experiments!</p> 
<p>Drexler also mentions additional terms that could be added to bonds under large tensile loads and nonbonded interactions under large compressive loads, but I believe that these are covered better in MM4, and ultimately, I am still skeptical of any computational model for such complex systems. </p> 
<p>(New Section) Potentials for chemical reactions </p> 
<p>Relationship to other methods: Molecular mechanics methods are based on the idea of structures with well-defined bonds; they cannot describe transformations that make or break bonds and cannot predict chemical instabilities. You could use CI methods to try and model bond cleavage and formation, but many papers also just rely on approximate potential surfaces.</p>  
<p>Bond cleavage and radical coupling: There’s not much interesting content here, just the basics: a simple reaction can be the bond cleavage of a dimer or a two-atom system that splits it into two radicals, but to combine two radicals into one, they would need paired, antiparallel electron spins to do bond formation. This doesn’t affect Drexler’s work very much; I’m not sure why he included this, to be honest. </p> 
<p>Abstraction reactions: Abstraction reactions are ones where reactions make and form bonds simultaneously, such as in the symmetrical hydrogen abstraction reaction: H3C + HCH3 -> H3CH + CH3. This doesn’t immediately relate to Drexler’s designs either, but I want to include as much information as I can. </p> 
 
<p>Continuum representation of surfaces: Van der Waals attractions between nanometer-scale objects can be substantial. (True!) Honestly, I think that using the Hamaker constant, which Drexler does, is typically used for continuum surfaces. </p> 
<p>Transverse-continuum models of surfaces: For non-metallic surfaces (which would undergo bonding rather than repulsion), we need different models for determining how two sliding, unreactive, atomically precise surfaces interact with each other. Drexler claims that if we neglect elastic deformation resulting from interfacial forces, it results in a surface-surface potential that could roughly look like “two planes rubbing against each other.” This is incorrect, as the surfaces on the atomic level would be much too small (although quite smooth enough) in order to make the assumption that we can neglect elastic deformation. What this implies is that we’ll need more computational power, obviously, but what designs now become unfeasible is uncertain. 3D bulk mechanosynthesis and motors should be relatively untouched, but rod logic computing and other methods that rely on surfaces sliding past each other should be looked at more carefully. There is the phenomenon of superlubricity (very low frictions at the atomic scale). Don’t think of superconductivity or superfluidity; the friction still exists; it’s just very small (like <0.01 for its kinetic coefficient of friction), which was theorized in Nanosystems and was experimentally shown to exist in 2004 (https://en.wikipedia.org/wiki/Superlubricity). That phenomenon could prove Drexler’s designs right, but I suspect that only the experiments will show how things really play out. </p> 
<p>Molecular models and bounded continuum models: For irregular surfaces, you really should be modeling at the level of interatomic potentials, but for bulk surfaces, Drexler suggests that one could try using bulk continuum models. This is developed more in Chapter 9, but I’m skeptical because these sorts of systems are so far out of the way of what exists even in nature (they’re much better than ribosomes, so don’t make that analogy), so I would just try to model all of the interatomic potentials if possible. </p> 
<p>Conclusions: The potential energy surface of a ground-state system can determine its mechanical properties, but (author’s note) I think that PES obviously isn’t enough to map everything out. For starters, everything is assumed to be stable, all transition states are mapped out, there are many approximations made, and many different surfaces may be needed for non-adiabatic events, but despite that, it’s a good starting point. Drexler does address some of these concerns later on (mostly the transition states and everything assumed to be stable), but I still have mixed feelings about using potential energy surfaces. </p> 
<p>Further reading: Drexler suggests additional resources for reading up on molecular quantum mechanics and potential energy surfaces, but honestly, I don’t think that will solve the problems inherent with trying to use computers to model these nanomechanical systems: in the end, you have to run the experiments. Working nanotechnology is still very much an empirical rather than a purely computational or theoretical discipline. </p>

<h1> Volume 2: Back From The Desert </h1>
<img src="images/vol_2_cover.png" alt="volume 2 cover">
  
<h1 id="#ch4">Chapter 4: Molecular Dyanmics</h1>
<h1 id="#ch5">Chapter 5: Positional Uncertainty</h1> 
<h1 id="#ch6">Chapter 6: Transitions, Errors, and Damage</h1> 
<h1 id="#ch7">Chapter 7: Energy Dissipation</h1> 
<h1 id="#ch8">Chapter 8: Mechanosynthesis</h1>
<h2>Volume 3: </h2> 
<p>Coming soon!</p>  
<h1 id="#ch9">Chapter 9: Nanoscale Structural Components </h1>
<h1 id="#ch10">Chapter 10: Mobile Interfaces and Moving Parts</h1>
<h1 id="#ch11">Chapter 11: Intermediate Subsystems</h1>
<h1 id="#ch12">Chapter 12: Nanomechanical Computational Systems</h1>
<h2>Volume 4: </h2> 
<p>Coming soon!</p>  
<h1 id="#ch13">Chapter 13: Molecular Sorting, Processing, and Assembly</h1>
<h1 id="#ch14">Chapter 14: Molecular Manufacturing Systems</h1>
<h1 id="#ch15">Chapter 15: Macromolecular Engineering</h1>
<h1 id="#ch16">Chapter 16: Paths to Molecular Manufacturing</h1>
<h1 id="#appendixa">Appendix A: Methodological Issues in Theoretical Applied Science</h1>
<h1 id="#appendixb">Appendix B: Related Research</h1>






  
  <h1 id="experimentalplan">Experimentation Plan</h1>  
  
  <p>“Gentlemen, we have run out of money. It is time to start thinking.”—Ernest Rutherford </p> 
  <p><b>3/29/24 NOTE: There are substantial ongoing changes to this experimental plan, and the most recent updates will be in a signal group chat. Please DM or email me your signal if you have relevant expertise and seek to be added.</b> </p>
<p>I would like to start by noting that this experimentation plan is subject to changes, but I will include all of those in the Update Logs section of this review. </p>
<p>Before any experiments begin, one must find a location to conduct the experiments.</p>
<p>I originally thought of going to various condensed matter physics labs around Stanford’s campus, but quickly realized that they would not be ideal because of accessibility issues since they often have many other experiments to run on their expensive machines. </p>
<p>Therefore, I then decided to investigate whether ES3 (End Station 3), a former SLAC end station that houses the Stanford Student Space Initiative (SSI), a space project engineering club that I’m involved in, could host these experiments. There certainly was enough physical space for the experiments, but the amount of regulations required, particularly around cryogenics, extreme vacuums, and nanomaterials (not including any high-temperature experiments), led me to believe that it would be easier to find a space in Palo Alto to work where I would be more unencumbered. Furthermore, ES3 is going through a renovation at the moment, and at the end of 2024, we (SSI) will have to move our stuff from the bottom floor into the 2nd floor room, but that will not be a major concern for the upcoming spring, summer, and fall quarters. </p>
<p>This is not a refusal of safety norms, as I do in fact believe safety should be my number one priority, but rather of burdensome regulations that would slow down these exciting experiments. I currently have an initial location in the basement of a friend in Palo Alto where I could conduct my experiments, but it is quite small both in size and particularly in height, and while I am extraordinarily grateful for my friend for thinking of me, I believe that I will need both additional space (x, y, and z directions) and have it not be in a home since roughing, diffusion, and cryopumps can be quite loud even with insulation-based dampening, and I would not like to irritate my landlord. </p>
<img src="images/basement1photo.png" alt="Basement 1 photo"> 
  <img src="images/basement2photo.png" alt="Basement 2 photo"> 
  <img src="images/basement3photo.png" alt="Basement 3 photo">
<p>I then decided to look at various locations in Palo Alto (ideally storage space or old warehouses), but the rent and lease conditions were extremely high and quite restrictive, respectively, and Stanford has been very fast in getting back to me in regards to EH&S (Environmental Health and Safety) training and discussions. Therefore, in the essence of saving money, travel time, and having enough X,Y, and Z direction space to fit all of my equipment, I will be conducting my experiments in ES3. In addition, our satellite team for SSI already needs to acquire a clean room. So, if I double-use my clean room for doing cubesat electronics assembly, then I will be able to have additional help for setting everything up and maintaining my equipment, which is non-trivial. </p>
 
<p>Now that I’ve decided on working in ES3 at Stanford, here are all of the legal and paperwork requirements I will need to work through: </p>
<p>SSI Safety Officer (Leo Glikbarg) uses stanford.bioraft.edu for reporting everything once per quarter, and SSI does our own self-inspections. </p>
<p>I’ll be in the Hansen Experimental Physics Lab South (HEPL) B10.</p>
<p>The safety manager is Michael.</p>
<p>There’s a site for tracking gas chambers for buying and maintaining the cylinders, which I’ll plan to use for my gases. </p>
<p>No food, drink, or cosmetics. </p>
<p>Buddy Rule: No Working Alone. (I’ll have other folks in ES3 or I’ll have Luke or another palo alto person come be in ES3 with me during unusual hours; I take that seriously.)</p>
<p>Equipment needs to have earthquake bracing. (This is important, and I’m not sure how I’ll do this with my clean room and other stuff yet, but I can get help from EH&S on this.) </p>
<p>The Fire Marshall has a quarterly tour, which is typically fine. </p>
<p>Make sure to wear proper PPE!</p>
<p>Daisy Chaining power strips is extremely bad, DO NOT DO THAT!  </p>
 
<p>The EH&S training that I’ll need to do on Axess and in person for everything will be these: </p>
<p>EHS-4200 (General Safety & Emergency Preparedness): Have Taken Already (Good).</p>
<p>EHS-1900 (Chemical Safety for Laboratories): Have Taken Already (Good). </p>
<p>EHS-2480 (Cryogenic Liquids and Dry Ice Safety) I am currently taking the quiz, but I need to do an in-person section soon. (In progress) </p>
<p>I might also need training for EHS-5500 (Tier III Safety Training for Chemical Lab Research, primarily because of using HF), laboratory-specific training (just due to the high complexity of experiments), and radiation-specific training (due to ion beams), but I am uncertain about both of these, so I will reach out to EH&S about this. </p>
 
<p>Now I will also need to include fluid diagrams (perhaps this was just mentioned to me by Max Newport, who had gone through EH&S for rocket propulsion experiments before) in order to create SOPs (standard operating procedures) for EH&S. I’m likely going to go to Professor Mark Cappelli, who helps to run the engineering physics program here at Stanford, since he runs a plasma physics lab that has been very friendly to SSI, and I believe he will be a valuable resource for how to write the SOP, so that way I can remain in good standing with EH&S. </p>
 
<p>The secondary non-technical concern with using ES3 as a workspace would be intellectual property (IP) concerns, which I believe to be a non-issue for the following reasons: </p>
<p>1.  I am not funded by Stanford. </p>
<p>2. I am planning to release this work on ArXiv rather than try to commercialize it. at least for these experiments and the funding proposed. After that, there might be more conversations to be had with ES3 and others.</p>
<p>3. The author says, “Title to all potentially patentable inventions conceived or first reduced to practice in whole or in part by members of the faculty or staff (including student employees) of the university in the course of their university responsibilities or with more than incidental use of university resources, belongs to the university." (https://otl.stanford.edu/researchers/intellectual-property-basics/stanford-policies-intellectual-property) </p>
<p>4. However, I will continually reach out to the offices here to ensure that he is fine.</p>
<p>Now that I’m done with describing the necessary paperwork, it’s time to describe what experiments I’m actually planning to run and the equipment I’ll need to run them. (Copious usage of eBay will occur.)  </p>
<p>The first experiment that I want to run would be a repeat of the famous 2003 nanoindentation paper using an nc-AFM in order to pick up and place silicon atoms on a Si(111)-7x7 surface. (https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.90.176102) They used n-doped silicon with As as their base for doing everything, but from this PhD thesis, it seems that the difference between p-doped and n-doped silicon here for Si(111)-7x7 shouldn’t make that much of a difference since I’m not making integrated circuits out of the wafers. (https://purehost.bath.ac.uk/ws/portalfiles/portal/189841519/ThesisCombined_Final_submitted_corrections_margins.pdf ) </p>
<p>The procedure I would be following would be as close as possible to this (including the home-built low-temperature ultrahigh vacuum NCAFM, although it’s possible we’ll be below 78K). With the exception of that, the tips for my AFMs are likely going to have different harmonic mechanical resonant frequencies.</p>
<p>“The experiments were performed using a home-built low-temperature (LT) ultrahigh vacuum (UHV) NCAFM [20] with a base pressure of 5 1011 Torr. NCAFM was operated in the frequency modulation detection scheme [21], and constant excitation mode [22] was used for oscillating low-resistance commercial n-doped silicon cantilevers with a typical spring constant of 48 N/m and a first harmonic mechanical resonant frequency of 160 kHz. The measured quality factor in UHVat 78 K was 170 000. Before cooling down to LT and starting the experiments, both the cantilever tip apex and sample surface were prepared in situ. A clean Si111-7x7 surface was obtained from an n-type As-doped single crystal Si wafer by direct current heating, flashing the sample up to 1200 C, and then slowly cooling down from 900 C to room temperature (RT). During sample preparation, vacuum pressure was kept below 1/1010 Torr. The Si tip apex was carefully cleaned up by argon-ion bombardment for 30 min. with 0.6 keV ion energy, 1 106 Torr of partial Ar pressure, and a normal incidence ion beam directly on the tip apex. The experiments were conducted at 78 K tip and sample temperature. During the experiments, both tip and sample were always electrically grounded.” </p>
<p>The direct current heating, flashing, and then subsequent cooling to room temperature don’t seem like they would be that tricky, but the argon-ion bombardment and partial Ar pressure are things that I am not very familiar with, but I think that Professor Cappelli would be able to help with them. In addition, preventing outgassing (https://adms.fnal.gov/vacuum/vacuumcertificationwhy.html) is something quite important but is an area where I have little experience at the moment, but I believe I’ll have time to pick that up since there’s a lot of refurbishment needed for the bell jar UHV system I’ve acquired. </p>
<p>This experiment would likely be extended into “experiment 1.5,” where the famous IBM “A Boy and His Atom”-style videos could be constructed, since I find it quite disappointing that more movies like that haven’t been made. (Granted, it was 100% novel only once, but people didn’t stop animating just because Steamboat Willy came out.)</p>
<p>Now, the second experiment that I would run would involve atomic layer deposition (ALD) onto the Si(111)-7x7 surface, likely of some kind of hydrocarbon or carbon-containing substance, that would form a layer with rigid tripodal portions of the molecules exposed for a tooltip to come in and manipulate them to form vertical 3D structures more easily. I think that trying to have the AFM tip crash into the surface in order to replenish the tip is not feasible for high-volume manufacturing, and while this experiment is still quite speculative (since nobody’s been able to make a more complicated 3D structure than a copper trimer (Cu3) with an AFM (https://www.science.org/doi/abs/10.1126/science.aaa5329)), I suspect that getting the ALD right and easy for tooltips to replenish feedstocks will be easier than trying to make 5 separate breakthroughs on the AFM.</p>
 
<p>Here’s stuff I’ve picked up: </p>
<p>UHV Bell Jar Chamber: This is stainless steel and has a nice rectangular viewing port in it, but it does have some dirt that needs to be cleaned off of it. There’s nothing toxic on it; we did a UV light check, and there’s no barium or cadmium on it that could be concerning. </p>
 
<p>Roughing Pump: It takes 220V, so I’ll need a 120V–220V step-up transformer. It should work, but we’ll need to test that. </p>
 
<p>Diffusion Pump: I don’t know what model this is, but it fits the UHV bell jar chamber perfectly, although I’ll want to do an oil replacement. </p>
 
<p>Feedthrough valves: I got a variety of them from Martin’s semiconductor warehouse, so we’ll see how many of them are good, but we’ll want RF signals (50 ohm impedance-matched coaxial cable) along with power to go into our chamber because the AFM will need those. </p>
  <img src="images/roughing_pump_and_feedthroughs.png" alt="roughing and feedthrough">
  <img src="images/diffusion_pump.png" alt="diffusion pump">
  <img src="images/bell_jar_1" alt="bell jar 1">
  <img src="images/wide_shot_bell_jar_2.png" alt="bell jar 2">
<p>Here’s the stuff I’ve ordered: </p>
<p>25 P-Type Si (111) wafers </p>
<p>1 gallon, 100% acetone </p>
<p>1 gallon of 99.8% isopropyl alcohol </p>
 
<p>Here’s the stuff I need to order: </p>
<p>Non-Contact Atomic Force Microscope (nc-AFM): There’s a variety of models on eBay, but the problem with them is that they don’t all come with the right parts, and some are quite vague about whether they have a nanopositioner built in or not. The latter point is very important, because if there was one built in, then that would be quite nice, as it’s one less variable to keep track of in my process.</p>
 
<p>Hydrofluoric (HF) Acid: I’ll get it from Sigma-Aldrich or a similar place. I would’ve ordered it now, but I want to double check with EH&S about shipping potentially dangerous chemicals. This paper (https://www.sciencedirect.com/science/article/pii/S0169433217332191) says that 1.5% concentration seems to be optimal. </p>
 
<p>Anti-Vibration Table: See, I don’t want to spend a ton of money on this (either by donation or retrofitting existing tables, etc.), but ES3 sometimes shakes because the concrete carries vibrations from heavy machinery quite easily, so I would want this to be isolated. </p>
 
<p>Modular Clean Room: I am not sure whether I want it to be a soft or a hard clean room, since ideally I would want an ISO 6 clean room (I don’t think ISO 5 or lower would be realistic to maintain, although every little bit helps). I suspect that acetone and ISA cleaning combined with stringent PPE would be able to get the vast majority of the benefits, along with some HEPA filter modding. The thing that concerns me about soft clean rooms is that they claim very low ISOs, some of them ISO5 and ISO6, but I’ve read in multiple sources that you’d want to do an airlock or decontamination to go from open air to clean rooms of those levels. (ISO 7/8 seems to be just fine.) Prices are probably going to be around $8–12k for these, with higher prices for better rooms. I also considered making my own soft clean room because it’s just a HEPA filter, lights, aluminum extrusions, some bolts, and vinyl sheets, but this is one area where I really want to have it be good, and SSI satellites would benefit a lot from having a real clean room for once. </p>
 
<p>Everything for the clean room: sticky blue mats for shoes, shoe covers, some kind of locker to keep PPE in (it needs to be earthquake-resilient though), gloves, masks, and coveralls should be enough. I’m still waffling over whether I should get a handful of reusable suits or just a bunch of throw-away PPE. I think this is a point I would love to get some advice on. </p>
 
<p>Ionization Gauges: You could do a DIY version like Mike suggested, but I would rather try and find some donated ones or go on eBay for a discount since I would prefer my gauges to be something I know “just works.”. </p>
 
<p>Thermocouples: I’ll just go on eBay to get these for my higher pressure regimes so that my ionization gauges don’t get damaged. (Cold cathode ionization gauges could technically survive room-pressure environments while working, but I think it’s just good practice.) </p>
 
<p>Pre-roughing pumps: This should get us from 1 atm to 0.1 atm before the roughing pumps begin, and I think that would be a good idea to get. eBay should have a lot of these. </p>
 
<p>Cryopumps: I think we’ll need these in order to really try and hit 10^-12 Torr rather than just 10^-8/10^-9 Torr. We have enough ports on the bell jar chamber to do this, so I think it’s worth it. </p>
 
<p>Cryo-coolers: Mike suggested that Gibson McMahon cryocoolers would be what we would want, and those can get fairly cold (like even down to 10–20K). For our experiments, the papers worked at 78K, which is liquid nitrogen temperature, but the colder we can get, the better since then we don’t have the atoms move around as much. One part I’m still not sure about with the theory is whether we would need liquid nitrogen to refill this (I don’t think so, since it’s not an icebox model cryo-cooler), but that would change EH&S SOPs, so this is a priority to talk to Amir and other vacuum experts about. </p>
 
 
<p>Step-Up Transformers: I believe I’ll need at least a 120V (wall power in ES3) to 220V step-up transformer (for the roughing pump), but I’ll want to check the voltage for the diffusion pump and other equipment to order ahead of time rather than trying to have everything delayed. </p>
 
<p>Magnehelic-Pressure Monitor: For verifying that the clean room is actually at the ISO level that it states. </p>
 
<p>Seals, Copper Gaskets, O-Rings, Flanges, Fittings, etc.: This is dependent on doing bell jar measurements, but I’m assuming that these will essentially all need to be replaced. I could get these donated or just go on eBay and match sizes (typically in inches). </p>




<p>Here’s the stuff I want to do more measurements on before ordering, but I think I’m going to need them. </p>
<p>You can get a qPlus or a Kolibri sensor to increase the imaging capability of your AFM. For my first experiment, I don’t believe this will be needed, but qPlus is the new top standard for AFMs, so I will be adopting that once more pressing matters are set up. </p>
<p>Laser Interferometers: I won’t need these for a while, but it would be nice to not need to switch back to a CO-tip for imaging and to just have my AFM send it full throttle. </p>
<p>Helium Leak Detector for my UHV: This is one I’m quite confident I’ll need, but I don’t know much about what models or specifications I would want this to have, so that’s an active area of research for me. </p>
<p>Space Blankets for Insulation: I originally thought I would want these for my UHV chamber in case of bake-outs, but the stainless steel aspect might imply I don’t need them. However, I think it would be good for very high annealing temperatures to have them. </p>
 
<p>What are my big bottlenecks at the moment? </p>
<p>Having all of the measurements of my UHV chamber, I’ll be able to size and order everything else once funding comes in (as well as regulations). Matt Parlmer is bringing the UHV chamber and the diffusion pump over from Martin’s warehouse on Wednesday, March 20th, at 11 a.m. to the ES3 loading dock (which is by Durand and is quite convenient). </p>
 
<p>I also want to measure the speed of the diffusion pump, the roughing pump, and any future pumps I am going to get as soon as possible, since there’s a lot of sources on the internet that discuss how long it can take to get a UHV down to low pressures, and however I can either speed that up (preferred) or at least plan ahead with the given time (somewhat more likely) would be good. </p>
 
<p>“Why doesn’t this proposal have any computational elements? Isn’t it typical of SPM academic work to have an experimental and a computational (typically DFT) component?” </p>
<p>Dear reader, you would be correct, but I believe that the majority of the uncertainty remaining in testing Drexler’s ideas is related to the experiments rather than the computations. There have been many computational papers written about APM that lead me to believe that my marginal contribution would be much greater on the experimental side. Furthermore, many complaints against the feasibility of APM rest on the idea that “yes, the computer simulations work, but you haven’t built a single one of Drexler’s designs yet. I’ll believe it when I see it.” </p>
<p>However, if I were going to run computations, then it would be roughly like how this paper (the Si(111)-7x7 did it; from my minor experience with DFT, these seem like good assumptions, but I will ask some of my academic advisors about their specific thoughts) did it: “Ab initio simulations of Co adsorption on Si(111)−7 × 7 surfaces were performed by DFT calculations using the QUANTUM ESPRESSO code. The PBE-GGA for the exchange correlation energy was adopted. Wave functions were expanded into plane waves with a kinetic energy cutoff of 300 Ry. A Si(111)−7 × 7 surface was modeled using a slab (7 Å thick) back-terminated with hydrogen atoms. A minimum vacuum space of 15 Å separated the atoms of one supercell from the atoms of the next repeat unit in the direction normal to the surface.” (https://www.sciencedirect.com/science/article/pii/S0039602822001157#:~:text=The%20Co%20atoms%20adsorbed%20on,by%20NC%2DAFM%2FKPFM.) </p>
<p>Now that I’ve described the experiments that I’m planning to run and their equipment set-ups, I’d like to discuss how I’m going to fund all of this. </p>
<p>First, equipment giveaways from Martin (with shipping help from Matt Parlmer in his U-Haul) have been invaluable, and if anyone is looking to support this project, then free equipment will always be appreciated if it is close to Palo Alto. </p>
<p>The funding I believe will be necessary for this project is roughly around 45k or so, divided up from these sources: </p>
<p>~1k: 1517 Medici Grant (Approved) </p>
<p>~5k: Personal Savings (Approved) </p>
<p>~10k: Z Fellows Award (need to apply)</p>
<p>~30k: Emergent Ventures Grant (need to apply) </p>
<p>~5k: Eclipse Grant (I need to apply; I'm uncertain about this one because it’s not necessary to run my experiments on it, but it would give some additional breathing room.) </p>
<p>If I need more money, I will set up a donation site where folks could either donate money or equipment of any amount, as long as it is relevant. If I decide to need a donation site, all donations will be placed in a public Google Doc where it is broken down where your money is going for transparency, since if you are giving me money or resources, then I want to respect your generosity. </p>
<p>If there are additional wealthy patrons who would enjoy sponsoring this basic research, I would also be very grateful, but I would not like to have that as my “Plan A."</p>

  <h1 id="updates">Update Log</h1>  
  <p>Right now there are no updates besides minor cosmetics, but there will be many for the experimental section and then there will be an update for Volume 2.</p> 
  <p>If I make any egregious errors, I will credit you for finding it so that we can fix all of them.</p>

  
  <h1><a href="index.html">BACK TO HOME</a></h1>
</div>

</body>
</html>
