<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      font-family: 'Times New Roman', Times, serif;
      line-height: 1.6;
      color: #333;
      background: #f8f8f8;
    }
    .container {
      width: 75%; /* Adjusted width to match the painting size */
      margin: 3em auto;
      padding: 2em;
      box-shadow: 0 0 1em rgba(0,0,0,0.05);
    } 
    h1 {
      text-align: center;
      margin-top: 0.5em;
      margin-bottom: 0.5em; 
      font-size: 3 em;
    }
    p, h2, a {
      margin-bottom: 1em;
      text-indent: 1.5em; /* Added text-indent for paragraphs */
    }
    a {
      text-decoration: none;
      color: #0077cc;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      width: 60%; /* Adjusted image to be smaller than the container */
      height: auto;
      display: block;
      margin: 20px auto; /* Increased top and bottom margin for better spacing */
    }
    @media (max-width: 768px) {
      .container, img {
        width: 95%; /* Full width for smaller screens, for both container and image */
      }
      p, h2, a {
        text-indent: 0; /* No indent for paragraphs on smaller screens */
      }
    }
  </style>
</head>
<body>

<div class="container">
  <h1>A Technical Review Of Nanosystems</h1> 
  <h1>By Jacob Rintamaki</h1>
  <img src="images/nanofactory.jpg" alt="Broadway Boogie-Woogie">   
  <h1>Acknowledgements</h1> 
  <p>I’d like to start by thanking Luke Farritor, who has spent many late nights assisting me with this review, Amir Tarkian, who has been extremely knowledgeable on matters of extreme vacuums, and Anton Troynikov, who gave me the idea of shipping something rather than nothing. :)</p>  
  <p>I’d also like to thank My Current [Anonymous] Landlord, Enze Chen, Adam Marblestone, Aydin Gocke, Teddy Ganea, Darson Chen, Martin, Matt Parlmer, Victor Li, Lachlan Sneff, Lukas Suss, Mark Friedenbach, Leo Glikbarg, and Max Newport for their support. </p> 
  <p>If there is anyone I have forgotten, I profusely apologize. I could not have done this/continue to do this alone.</p>
  <h1>tl;dr</h1> 
  <p>The math of Nanosystems seems to work enough to be interesting (even if its not as fast/strong/etc. as the book suggests), yet only a handful of relevant physical experiments have been published.</p>
  <p>I’m going to go do more of these relevant physical experiments and put the results on <a href="https://arxiv.org">arXiv</a>.</p>
  <p>My first experimental goal is to recreate <a href="https://pubmed.ncbi.nlm.nih.gov/12786084/">this paper</a> (where the authors are able to remove and deposit Si atoms on a Si(111)-7x7 surface using an NC-AFM at 78K under UHV conditions).</p>
  <p>I would ideally like to have recreated this experiment by June 19th, 2024 (3 months from the publication of this review), but that's <b>very, very, very, very ambitious.</b></p> 
  <p>However, sometimes <b>very, very, very, very ambitious</b> timelines can enable us to achieve the near-impossible.</p>
  <p>If you think you could help me achieve this <b>very, very, very, very ambitious</b> goal (via advice, connections, equipment, money, etc.) please email me at <a href="mailto:jrin@stanford.edu">jrin@stanford.edu</a> or DM me on Twitter/X at <a href="https://twitter.com/jacobrintamaki">@jacobrintamaki.</a></p>
  <p>Back to work now.</p>

  <h1>Table Of Contents</h1>   
  <h2>Volume 1:</h2>
  <h2><a href="#motivation">Motivation</a></h2> 
  <h2><a href="#drama">Drama</a></h2> 
  <h2><a href="#technicalprogress">Technical Progress</a></h2> 
  <h2><a href="#preface">Nanosystems Preface</a></h2>  
  <h2><a href="#ch1">Chapter 1: Introduction and Overview</a></h2> 
  <h2><a href="#ch2">Chapter 2: Classical Magnitudes and Scaling Laws</a></h2> 
  <h2><a href="#ch3">Chapter 3: Potential Energy Surfaces</a></h2> 
  <h2><a href="#experimentalplan">Experimental Plan</a></h2> 
  <h2><a href="#updates">Update Log</a></h2> 
  <h2>Volume 2: </h2> 
  <h2>Coming soon!</h2> 
  
  <h1 id="motivation">Motivation</h1>  
  
  
  <p>When you hear the word "nanotechnology," what do you think of?</p>
  <p>To many materials scientists, the word nanotechnology conjures ideas of carbon nanotubes and ever-more-advanced semiconductor fabrication plants trying to cram billions of transistors onto AI chips.</p>
  <p>To some science fiction fans, nanotechnology looks more like a swarm of self-replicating robots that devour everything—a powerful predator.</p>
  <p>But to Eric Drexler, the author of Nanosystems, nanotechnology doesn't look like carbon nanotubes or killer robots but rather a super-fast 3D printer that could make many non-edible physical products atom by atom.</p>
  <p>Drexler calls this vision of nanotechnology APM (atomically precise manufacturing), and if it were invented, it would be one of the most important general-purpose technologies ever.</p>
  <p>Here's an example: if APM worked according to Drexler's calculations, then APM could produce non-edible physical products ten times faster than traditional manufacturing while also having the prices be ten times cheaper. Even better, the product would be of ten times higher quality than its original, typically manufactured version, while also being more sustainable than practically every other manufacturing process we have today. Finally, to make things even more absurd, it's likely that the "ten times" metrics I mentioned earlier are underrating the full potential of APM.</p>
  <p>Let me repeat that again: it's likely that the "ten times" metrics I mentioned earlier are UNDERRATING the full potential of APM.</p>
  <p>That's incredible, but we've only scratched the surface of what APM can do.</p>
  <p>In stories and research papers written by futurists such as <a href="https://bigthink.com/videos/ray-kurzweil-on-the-future-of-nanotechnology/">Ray Kurzweil</a>, <a href="https://mason.gmu.edu/~rhanson/nanoecon.pdf">Robin Hanson</a>, <a href="https://en.wikipedia.org/wiki/The_Diamond_Age">Neal Stephenson</a>, <a href="https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination">Eliezer Yudkowsky</a>, <a href="https://nickbostrom.com/existential/risks.pdf">Nick Bostrom</a>, and others, APM could bring about an age of abundant wealth and scientific knowledge, allowing us to cure diseases, build ultra-powerful computers, and harness the full power of our sun for energy. However, some of these futurists warn of how APM could be utilized (particularly by advanced AI systems) to <a href="https://www.youtube.com/watch?v=Yd0yQ9yxSYY">wipe out</a> all of <a href="https://nickbostrom.com/existential/risks.pdf">humanity</a>, but luckily <a href="http://apm.bplaced.net/w/index.php?title=Molecular_assembler_(disambiguation)">I don't think we have to worry about APM wiping out humanity.</a></p>
  <p>Yet, taking a step back, despite all of this attention from prominent tech futurists, there's surprisingly been very little technical content written about the true feasibility of APM in recent years, and what has been written is mostly of low quality. To make matters worse, there have also been almost no physical experiments with the goal of validating APM published in recent years.</p>
  <p>Now, this doesn’t mean that APM is impossible, since the theoretical foundations for computers and rockets were developed many decades before practical versions of either were built, but there’s still a lot of work left that has to be done to achieve APM.</p>
  <p>So then, let’s get started; we don’t have a moment (or atom) to waste.</p>

  <h1 id="drama">Drama</h1>  
  <p>A common criticism of Nanosystems is that, despite being visionary, there’s been very little experimental progress towards APM since it was published in 1992.</p> 
  <p>(Now, this doesn't inherently mean that the technology is bunk, since both computers (Babbage, Lovelace) and rockets (Tsiolkovsky) had their theoretical foundations somewhat established decades before working versions were built, but it does inspire skepticism in me about the feasibility of APM.)</p>
  <p>The next section of this review covers the last 30 years of technical progress (it’s mixed) towards APM, but there was a large social element that derailed Drexler’s vision.</p>
  <p>This section focuses on that social element, which started in 1986, when Drexler published the popular science book <a href="https://philpapers.org/rec/DREEOC">"Engines of Creation"</a> that he wrote to get the public excited about APM. In this book, Drexler brings up the now-infamous idea of “Grey Goo,” a catastrophe where nanotechnological replicators would exponentially self-replicate, consuming the entire Earth in the process.</p>
  <p>Drexler then discredited the idea immediately after, claiming that, while not impossible, it was inefficient, along with other reasons, but it was too late.</p>
  <p>In the year 2000, Wired magazine published an <a href="https://www.wired.com/2000/04/joy-2/">article</a> by Bill Joy, a cofounder of Sun Microsystems, that warned of the existential dangers of nanotechnology, along with robotics and genetic engineering, because they could self-replicate.</p>
  <p>Robert Freitas, a close collaborator of Drexler, then wrote a paper entitled <a href="https://www.rfreitas.com/Nano/Ecophagy.htm">“Some Limits to Global Ecophagy"</a>, which disavowed the idea of a surprise takeover from Grey Goo. (Freitas argues that these “Grey Goo” incidents are still possible, just that they would be easily detectable and preventable due to the large amount of heat they would generate.)</p>
  <p>However, this paper wasn’t enough, and the public’s fear of Grey Goo led Nobel Chemist Richard Smalley, famous for his discovery of Buckminsterfullerene (also known as "Buckyballs"), to write the article “Of Chemistry, Love, and Nanobots" for the September 2001 edition of Scientific American. In this article, which started the two-year series of back-and-forth correspondences known as the <a href="https://en.wikipedia.org/wiki/Drexler%E2%80%93Smalley_debate_on_molecular_nanotechnology">"Drexler Smalley Debate"</a>, Smalley argued that Drexler’s visions of nanotechnology were flawed for two reasons: sticky fingers and fat fingers. Sticky Fingers refers to Smalley’s claim that atoms will stick to the manipulator’s atom and thus mechanosynthesis is impossible, and Fat Fingers refers to the claim that there isn’t enough room for the 10 or so atoms required to do mechanosynthesis. <a href="http://www.imm.org/publications/sciamdebate2/smalley/">Drexler rebutted</a> the sticky fingers claim by citing the multiple experimental papers that have shown atom-by-atom placement as possible, and the fat fingers claim by saying that mechanosynthesis doesn’t require those 10 atoms in such a small space because that would be impossible, but rather just three or so (example of a CO-functionalized tip on an AFM). Drexler then wrote two additional letters (one in April due to his concerns about Smalley slandering his work and one in June calling Smalley out for not responding to him) to Smalley. Drexler and Smalley’s final debate was in 2003 with a <a href="https://courses.cs.duke.edu/cps296.4/spring08/papers/Drexler.v.Smalley.pdf">“Point-Counterpoint”</a> feature in Chemical & Engineering News (C&EN), where Smalley wrote that Drexler’s vision of grey goo was scaring the children while Drexler advocated for more systems research to develop nanotechnology.</p>
  <p>Although I believe that Drexler won the debate over Smalley, the funding for the newly founded National Nanotechnology Institute (NNI) went almost entirely to Smalley’s vision of nanotechnology, which focused more on chemistry done at small scales than Drexler’s vision of atomically precise nanomachines. Drexler’s 2007 <a href="https://eprints.internano.org/76/1/Productive_Nanosystems_07.pdf">Productive Nanosystems roadmap</a> was also not funded, leaving Drexler’s vision seemingly defeated.</p>
  <p>I would like to note that the loss of scientific momentum in the field of APM was arguably more important than the lack of funding, as APM was derided for many years as pseudoscience. As an example of this derision, Drexler’s 2013 book “Radical Abundance” was reviewed by C&EN (a popular chemistry journal) with the following title: <a href="https://cen.acs.org/articles/91/i30/Delusions-Grandeur.html">“Delusions of Grandeur:</a> [The] author insists ‘atomically precise manufacturing’ will transform civilization, but he’s not dealing with reality.”</p>


  <h1 id="technicalprogress">Technical Progress</h1>  

  
  <b>Here’s an additional list of related papers that may be of interest:</b>
  <p><a href="https://pubs.acs.org/doi/10.1021/acsnano.3c10412">Atomically Precise Manufacturing of Silicon Electronics</a></p>
  <p><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=-cuxoSQAAAAJ">Sergei Kalinin’s work</a></p>
  <p><a href="https://pubs.rsc.org/en/content/articlehtml/2014/nr/c3nr06087j">Fab on a chip original paper</a></p>
  <p><a href="https://open.bu.edu/handle/2144/41474?show=full">Fab on a chip PhD thesis</a></p>

  
  <h1 id="preface">Preface</h1>  
  <p><b>Drexler’s Big Idea:</b> The end state of materials science, chemistry, microtechnology, and manufacturing is the precise molecular control of complex structures. In other words, <b>molecular nanotechnology.</b></p> 
  <p><b>Drexler’s Guided by Many Readers:</b> General science background, a student interested in a career in nanotechnology, a physicist, a chemist, a molecular biologist, a materials scientist, a mechanical engineer, or a computer scientist.</p>
  <p><b>Interesting Point:</b> If you look at this from just one perspective, it looks wrong. For example, to pure chemists or pure mechanical engineers, molecular nanotechnology looks wrong, but holistically, it works.</p>
  <p><b>Criticism of Criticism:</b> Some critics see bad nanotechnology designs proposed and then use that to discredit the entire field of molecular nanotechnology, among other things. This is a poor mindset from would-be critics.</p>
  <p><b>Good epistemics:</b> Drexler then has a section saying that “will be” vs. “would be”; he’s not doing false advertising about devices that do or don’t exist.</p>
  <p><b>Thoroughness:</b> Drexler explicitly lays out his approach to citations as “Boltzmann isn’t cited, and neither is novel nanomechanical stuff” (paraphrased), but he tries to cite as much as possible.</p> 
  <p><b>Apologies:</b> Drexler apologizes for being the theorist in an experimentally heavy field, because that’s the hard work that has yet to come.</p> 
  <h1 id="ch1">Chapter 1: Introduction and Overview</h1>  
  <p>Chapter Goal: It’s just an introduction; nothing unusual here.</p> 
<p>A legendary opener: “The following devices and capabilities appear to be both physically possible and practically realizable:</p> 
<p>Programmable positioning of reactive molecules with ~0.1 nm precision.</p>  
<p>Mechanosynthesis at >10^6 operations/device * second </p> 
<p>Mechanosynthetic assembly of 1 kg of objects in <10^4 s</p> 
<p>Nanomechanical systems operating at ~10^9 Hz </p> 
<p>Logic gates that occupy ~10^-26 m^3 (~10^-8 mu^3) </p> 
<p>Logic gates that switch in ~0.1 ns and dissipate <10^-21 J </p> 
<p>Computers that perform 10^16 instructions per second per watt </p> 
<p>Cooling of cubic-centimeter, ~10^5 W systems at 300K </p> 
<p>Compact 10^15 MIPS parallel computing systems </p> 
<p>Mechanochemical power conversion at >10^9 W/m^3 </p> 
<p>Electromechanical power conversion at >10^15 W/m^3 </p> 
<p>Macroscopic components with tensile strengths >5 x 10^10 GPa </p> 
<p>Production systems that can double capital stocks in <10^4 s”</p> 
<p>Here’s the context and math to double-check, which will involve the use of future chapters.</p> 
<p>Programmable positioning of reactive molecules with ~0.1 nm precision. </p> 
<p>Given that mechanosynthesis involves moving things atom by atom, this is essentially a rephrasing of that. This has already been demonstrated in the technical progress portion of this review. </p> 
<p>Mechanosynthesis at >10^6 operations/device * second </p> 
<p>This is based on being in a vacuum, because while 10^6 operations/device * second sounds fast at first, if this is done atom by atom, then this would imply an assembly frequency of 1 MHz. Now, resonant frequencies for atomic force microscopes can be in the hundreds of megahertz (https://www.nature.com/articles/s41378-022-00364-4), and thus if every 100 out of the many vibrations per second that are occurring end up being able to pick up, place down, and then adjusting (imaging at the macroscale could theoretically be done with a laser interferometer instead of a CO-functionalized tip once feedback-force curves have been characterized enough, allowing for more breathing room, and I would also like to note that this is a rough approximation), it could be reasonable to do 10^6 operations/device * second. This would be more constrained for mechanosynthesis-designed AFMs at the nanoscale, but it’s not unreasonable. </p> 
<p>Mechanosynthetic assembly of 1 kg of objects in <10^4 s </p> 
<p>10^4 seconds = 10,000 seconds, or roughly 2.77 hours. In 14.7, Drexler’s calculations for convergent assembly add up to saying a 1 kg system can build a 1 kg product object in roughly 1 hour, dissipating 1.1 kW of heat. This argument primarily relies on an idea called convergent assembly rather than having one machine place down every single atom (even at rates of 10^6 operations per device * second, that would take 10^18 seconds for a device with 10^24 atoms, which is quite long!). This is one area where I’m actually skeptical about calculations given that we are not anywhere close to convergent assembly, so I’m leaving it in a state of limbo. Much of this argument rests on the idea that we’ve thought of the majority of frictional modes and other such factors that somewhat hinder nanomechanical computing in order to exponentially assemble macroscopic objects. </p> 
<p>Nanomechanical systems operating at ~10^9 Hz </p> 
<p>In chapter 14 (specifically 14.5.3), Drexler mentions devices operating at around 10^6 Hz and doesn’t seem to mention 10^9 Hz very frequently (although Chapter 10.10.2c mentions diamondoid nano-mechanisms operating for years at >10^9 Hz). However, if we take into account a fast AFM, then this figure doesn’t seem completely unreasonable (although more around 10^8 Hz), but I would rather stick to around 10^6 Hz as that’s what Chapter 14 uses for its convergent assembly math, and that’s less extreme than a billion ticks per second (1 GHz). </p> 
<p>Logic gates that occupy ~10^-26 m^3 (~10^-8 mu^3) </p> 
<p>Right now, as of 2024, we’re around the 3 nm “node” (semiconductor industry terminology that refers to a level of technology), which has about a 48 nm gate pitch and a 24 nm metal pitch. Roughly modeling a logic gate as a cube with 24 nm sides, that would be a volume of around 1.3824*10^-23 m^3. There’s still room to go with current nodes, so the volume of a logic gate could be ever smaller, but this would represent 1-2 orders of magnitude smaller logic gates. This claim isn’t as wild as Drexler’s other claims, but perhaps it’s wild that Moore’s Law has continued for so long. (FinFET, a pivotal technology, wasn’t even invented until 7 years after Nanosystems!). </p> 
<p>Logic gates that switch in ~0.1 ns and dissipate <10^-21 J </p> 
<p>Chapter 12 seems to be signal speed propagation of diamond/10 is like 1.7 km/s, and then scaling that down to the 1 um scale is 0.6 seconds. In Chapter 12.3.4d, it’s 0.05 maJ (milliatto-joule, or 10^-21 joules), which means that yes, it dissipates under 10^-21 joules. However, comparing this to modern logic gates, modern logic gates are usually on picosecond (10^-12 second) switching times, so rod logic computers don’t perform as well there, but modern computers are usually 10^-18 joules for switching, which means that rod logic could potentially have an advantage for energy efficiency. Personally, I suspect the main advantage of rod logic computing (Chapter 12) is physical compactness rather than speed or power efficiency, for which nanoelectronics could do quite well. Drexler is just wary of nanoelectronics because they do exhibit quantum effects, which make them non-trivial to model. </p> 
<p>Computers that perform 10^16 instructions per second per watt </p> 
<p>Chapter 14.12.9 conclusions mentions this, but I don’t see the explicit calculations being carried out anywhere in Nanosystems. Also, this is challenging because ISP/W doesn’t get used a ton nowadays as compared to FLOPs (floating point operations per second) due to the deep learning and GPU revolutions (Nvidia did not exist in 1992!). This would be many orders of magnitude more than current computers, even GPUs, however, and could mainly be attributed to energy efficiency rather than raw computational power, if I had to make a guess. I would leave the verdict on this one as “plausible, but very vague and needs a lot more math.” </p> 
<p>Cooling of cubic-centimeter, ~10^5 W systems at 300K </p> 
<p>In Chapter 11.5, Drexler directly addresses the cooling of cubic-centimeter, 10^5 W systems at 300K. First, it’s 273K (the melting point of ice) rather than 300K, and there are two interesting points being made. The first is that the design of distinct heat-carrying bodies parallels the use of red blood cells as oxygen carriers in the circulatory system. This is what the best of Drexler looks like to me: looking to biology for inspiration and then using deterministic manufacturing in order to build better systems for various purposes. The second interesting point is that from Heimenz’s 1986 “Principles of Colloid and Surface Chemistry,” an expression derived from the Einstein relationship relates the viscosity of a fluid suspension n, the volume fraction of spherical particles fvol, and the viscosity without the particles n0, as n/no = (1-fvol)^-2.5, and is reasonably accurate for fvol <=0.4. Packaged ice particles with a flow rate of 0.4 increase the fluid viscosity by a factor of 3.6, and if pentane is used as a carrier, the viscosity of the resulting suspensions is 8*10^-4 N*s/m^2. On melting, ice particles at this volume fraction absorb 1.2*10^8 J/m^3 based on coolant volume. Plugging this into the equation, this factor of energy dissipation makes sense, both mathematically and analogously. </p> 
<p>Compact 10^15 MIPS parallel computing systems </p> 
<p>MIPS is a family of RISC ISAs (and MIPS has announced in 2021 that it’s transitioning to the RISC-V ISA). Right now it’s Chapter 12 that mentions this in Nanosystems, and the conversion from MIPS (Million Instructions Per Second) to a more modern measure of computing power, FLOPs (Floating Point Operations Per Second), is around a million FLOPs (10^6) in one MIPS, but I would like to explicitly note that this is likely wrong for many reasons. One of the reasons this estimate would be wrong is that floating point operations can take more clock cycles to complete than integer operations, but also that many processing systems can conduct multiple floating point operations during their clock cycles due to various performance tricks like SIMD (single instruction, multiple data) or more. This is not my area of expertise, but if we did take that million FLOPS to one MIP comparison, then this would be 10^21 FLOPs in a compact zone, while Nvidia’s recent Blackwell B200 GPU is quite large and would deliver 20 exaflops (around 2*10^16 FLOPs per unit). (https://www.tomshardware.com/pc-components/gpus/nvidias-next-gen-ai-gpu-revealed-blackwell-b200-gpu-delivers-up-to-20-petaflops-of-compute-and-massive-improvements-over-hopper-h100) This is an area that would be a major use case for nanomechanical computing, as that kind of computing power in such a small space would be incredible, whether for medical nanorobots or other applications. </p> 
<p>Mechanochemical power conversion at >10^9 W/m^3 </p> 
<p>Chapter 14.5.5 points to Chapter 13.3.8. I immediately see problems with this, namely that it involves the reaction 2H2 + O2 -> 2H2O, which is normally explosive (it’s the reaction of LH2 and LOx that rockets—not all, because many use methane instead of LH2 these days, in part because you can create methane in situ electrochemically on Mars). Section 8.5.10: Transition-metal reactions is said to provide justification for this, but I’m not sure I see the logic here. Besides, mechanochemical power would not be worthwhile compared to electromechanical power conversion, so I’m not that mad at this being quite off. </p> 
<p>Electromechanical power conversion at >10^15 W/m^3 </p> 
<p>Chapter 11.7.3: Motor power and power density covers this section, and I am immediately skeptical for many reasons: 1) Drexler makes the assumption of “neglecting resistive and frictional losses,"  which is not something I would immediately do, and the power density is so large because the volume of the motor is incredible small and the stiffness of the diamondoid structures can support high centrifugal loads. I would shelve this under “not believable” IF viewed at the macroscale, because at the microscale these incredible densities would hold, but the entire amount of solar radiation that the earth intercepts is 10^17 watts, which would imply about 1000m^3 of these motors would be more than the entire sun hitting the Earth, which is absurd. </p> 
<p>Macroscopic components with tensile strengths >5 x 10^10 GPa </p> 
<p>I don’t like to leave a section like this because I’m assuming there’s something I clearly must be missing, but I haven’t seen this example specifically mentioned anywhere in Nanosystems. Off the top of my head, though, I would suspect that this is wrong (although likely much higher than what exists today) because it relies on continuum models at the nanoscale, which are then convergently assembled into macroscopic objects, thus making some assumptions that we shouldn’t. The strongest argument I could see for high tensile strengths would be that diamondoid is incredibly stiff, and if it (like carbon nanotubes, both carbon, by the way) were grown in long, macroscopic, thin tubes, then it would have an incredible tensile strength. </p> 
<p>Production systems that can double capital stocks in <10^4 s </p> 
<p>In chapter 14, for the conclusion, Drexler mentions that a 1kg system can build a 1kg product object in about 1 hour, dissipating about 1.1 kW of heat. This would be less than the roughly two and a half hours that 10^4 seconds would imply, and is perhaps the most astonishing thing besides the compact 10^15 MIPS computer that is on this first page of Nanosystems. The mathematics for how this is feasible is written in Chapter 14.4, and it doesn’t seem too unusual with the exception that even if the components operated at their stated frequencies (and then Drexler cuts it by three for a margin of error), that’s still potentially quite efficient. The problem with nanomechanical computing is that as it got studied, more frictional modes were discovered, reducing its efficiency. If this were not possible (doubling capital stocks in roughly an hour), it would likely be because the compounded efficiencies of the motors, gearboxes, sensors, etc. are much worse than Drexler makes them out to be. </p> 
<p>Definition Time! (Because this will be an issue 10 years after Nanosystems is written for Drexler-Smalley and other debates.) </p> 
<p>Molecular Manufacturing: “The construction of objects to complex, atomic specifications using sequences of chemical reactions directed by non-biological molecular machinery.”</p> 
<p>Molecular Nanotechnology: “...describes the field as a whole.”</p> 
<p>Mechanosynthesis: “Mechanically guided chemical synthesis” </p> 
<p>Machine-Phase Systems: A system in which all atoms follow controlled trajectories (within a range determined in part by thermal excitation). Machine-phase chemistry describes the chemical behavior of machine-phase systems, in which all potentially reactive moieties (a portion of a molecular structure having some property of interest) follow controlled trajectories. </p> 
<p>Important: Don’t confuse these with old fields; they’re new fields. </p> 
<p>CONTEXT: In 1992, positional chemical synthesis was at the threshold of realization. Eigler and Schweizer's 1990 paper, “Positioning single atoms with a scanning tunneling microscope,” (https://www.nature.com/articles/344524a0). However, advanced positional chemical synthesis is still in the realm of design and theory.</p> 
<p>Table 1.1: Some known issues, problems, and constraints </p> 
<p>Thermal excitation </p> 
<p>Chapter 5</p> 
<p>Thermal and quantum positional uncertainty </p> 
<p>Chapter 5</p> 
<p>Quantum-mechanical tunneling </p> 
<p>Chapter 5</p> 
<p>Bond energies, strengths, and stiffness </p> 
<p>Chapter 3</p> 
<p>Feasible chemical transformations </p> 
<p>Chapter 8, but this is scattered around a lot more. I’d still go primarily with Chapter 8 because that’s the mechanosynthesis chapter. </p> 
<p>Electric field effects </p> 
<p>Chapter 11</p> 
<p>Contact electrifications </p> 
<p>Chapter 11</p> 
<p>Ionizing radiation damage </p> 
<p>Chapter 6</p> 
<p>Photochemical damage </p> 
<p>Chapter 6 </p> 
<p>Thermomechanical damage </p> 
<p>Chapter 6</p> 
<p>Stray reactive molecules </p> 
<p>Chapter 6</p> 
<p>Device operational reliabilities </p> 
<p>Chapters 10 and 11 primarily state that a device doesn’t mean a macroscale system. </p> 
<p>Device operational lifetimes </p> 
<p>Chapters 10 and 11 primarily state that a device doesn’t mean a macroscale system.</p> 
<p>Energy dissipation mechanisms </p> 
<p>Chapter 7 </p> 
<p>Inaccuracies in molecular mechanics models </p> 
<p>Chapter 3</p> 
<p>Limited scope of molecular mechanics models </p> 
<p>Chapter 3</p> 
<p>Limited scale of accurate quantal calculations </p> 
<p>Chapter 3</p> 
<p>Inaccuracy of semiempirical models </p> 
<p>Chapter 3 </p> 
<p>Providing ample safety margins for modeling errors </p> 
<p>Does this throughout.</p> 
 
<p>Drexler seems to be fine with pointing out errors, with one exception: he assumes that failures happen fast and are easily detectable rather than being slower and more insidious. That was the one criticism Eliezer Yudkowksy made about Nanosystems (https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination), and it does look like a genuine blindspot. Drexler makes this “single point of failure” assumption because, on the atomic and molecular scales, if something goes wrong, it will cause operation failure quite quickly due to the small distances and high frequencies of atomic vibrations. However, as things scale, this doesn’t hold, and while Drexler eventually will mention radiation damage as breaking this “single point of failure” assumption, I don’t think he talks about it enough. To deal with this, we could have solutions for redundancy and monitoring and testing devices over long periods of time. Drexler already builds a lot of redundancy into his designs, so that’s fine. Adding internal sensors and additional forms of monitoring (internal and external) could be useful, but that feels like a further concern at the moment. </p> 
<p>Table 1.2: Contrasts between solution-phase and mechanosynthetic chemistry (Table 8.1 in Chapter 8 is more detailed) </p> 
<p>Comparisons: </p> 
<p>Conventional fabrication and mechanical engineering </p> 
<p>Similarities:Both have components, systems, controlled motion, and rely on traditional ideas of manufacturing rather than synthesis. </p> 
<p>Differences: APM will be much smaller and have to deal with molecular phenomena. </p> 
<p>Verdict: Agree with Drexler. At a high level, there are many similarities that we can borrow, but at a low level, these intuitions must be abandoned. </p> 
<p>Microfabrication and microtechnology </p> 
<p>Similarities: Both are on small scales and have quantum effects. </p> 
<p>Differences: Drexler claims that technologies like photolithographic pattern definition, etching, deposition, diffusion, etc. </p> 
<p>Verdict: I disagree with Drexler; I think that microfabrication and microtechnology will be significantly more important than he expects. However, this is because in the 30 years since Nanosystems was released, these technologies have rapidly improved and are nearing the atomic scale, although there are major issues and tradeoffs for precision, throughput, reliability, etc. </p> 
<p>Solution-Phase chemistry </p> 
<p>Similarities: Both rely on molecular structure, processes, and fabrication.</p> 
<p>Differences: Drexler focuses on mechanosynthesis and machine-phase systems. </p> 
<p>Verdict: Complete opposites, but some principles of solution-phase chemistry could be useful here. </p> 
<p>Biochemistry and molecular biology </p> 
<p>Similarities: Both have molecular machines (ribosomes in particular are a great model system and are used frequently in Drexler’s rebuttal arguments) and molecular systems. </p> 
<p>Differences: biology uses polymeric materials instead of diamondoid materials and works with stochastic, solution-based chemistry rather than the precise machine-phase chemistry Drexler proposes. </p> 
<p>Verdict: Ribosomes are an interesting model system, but I don’t think biology is a good proof of existence for machine-phase chemistry since the materials and environment are radically different. </p> 
<p>From Drexler: While full quantum electrodynamic (QED) simulations of nanosystems would be ideal, we must make assumptions for computational tractability because that’s how engineering works. </p> 
<p>Scope and assumptions: </p> 
<p>A narrow range of structures </p> 
<p>Author: Very fair. </p> 
<p>No nanoelectronic devices </p> 
<p>Author: Personally, I dislike this choice a lot.</p> 
<p>Machine-phase chemistry </p> 
<p>Author: Fair. </p> 
<p>Room temperature processes </p> 
<p>Author: Enjoy this. </p> 
<p>No photochemistry </p> 
<p>Author: Good choice can be fickle. </p> 
<p>The single-point failure assumption </p> 
<p>Author: Controversial, I addressed this previously. </p> 
<p>Objectives and non-objectives </p> 
<p>Not describing new principles and natural phenomena </p> 
<p>Author: Good choice.</p> 
<p>Seldom formulating exact physical models </p> 
<p>Author: To be fair, computational tractability is an issue.</p> 
<p>Seldom describe immediate objectives. </p> 
<p>Author: I disagree with this choice, particularly considering how much of a deviation from the traditional chemistry paradigm this is. </p> 
<p>Not portraying specific future developments </p> 
<p>Author: Fair.</p> 
<p>Seldom seek an optimal design in the conventional sense. </p> 
<p>Author: Maximizing efficiency is of secondary concern for the purpose of this book over derisking additional areas. Good choice.</p> 
<p>Seldom specifying complete detail in complex systems </p> 
<p>Author: The author understands why Drexler made this choice, but the author disagrees with it and believes it hurts the perception of nanosystems in the scientific community.</p> 
<p>Favoring false-negative over false-positive errors in analysis </p> 
<p>Author: Good choice.</p> 
<p>Describing technological systems of novel kinds and capabilities </p> 
<p>Author: Ok.</p> 

  <h1 id="ch2">Chapter 2: Classical Magnitudes and Scaling Laws</h1>  
  <p>Chapter Goal: “When used with caution, classical continuum models of nanoscale systems can be of substantial value in design and analysis.” (DISAGREE!)</p> 
<p>Approximation is inescapable. Why? The most accurate physical models (first-principles quantum mechanics) are computationally intractable. This gets discussed a lot in the next chapter. </p> 
 
<p>Basic assumptions for mechanical systems are: constant stress, which implies scale-independent elastic deformation; scale-independent shape; scale-independent speeds; and constancy of space-time shapes describing the trajectories of moving parts. </p> 
<p>Author’s Note: I do not agree with this assumption because size-determining unusual properties is a central idea of modern nanoscience (which is separate from APM).</p> 
<p>The choice of diamond as a material is interesting: density (3.5 * 10^3 kg/m^3), Young’s modulus (10^12 N/m^2), low working stress (10^10 N/m^2).</p> 
<p>What do these assumptions mean? “Larger parameter values (for speed, acceleration) relative to those characteristic of more familiar engineering methods.” Things move fast at the nanoscale. </p> 
<p>Is that intuition good? Yes. Biology is quite fast at the nanoscale (an analogy we’ll use and then not use in various sections of this review). Look at the website “Cell Biology by the Numbers” (http://book.bionumbers.org/) (a Laura Deming favorite) to learn more about this. </p> 
<p>Another big idea of this chapter is that “scaling principles indicate that mechanical components can operate at high frequencies, accelerations, and power densities.” </p> 
<p>Is this idea right? Yes, for the same reason the biology analogy I brought up in this chapter reviews earlier works. However, how high “high frequencies, accelerations, and power densities” are could be up for debate, since 10^6 Hz and 10^9 Hz are both high frequencies but are quite different things in practice. </p> 
<p>Adverse scaling of wear suggests that bearings are a special concern. This is a good point that Drexler brings up, since wear and friction are two areas that commonly get brought up as complaints against nanosystems being possible. </p> 
<p>Author’s Note: This is around the moment where I begin to severely disagree with Drexler, since for the next few sections he covers classical mechanical, electromagnetic, and thermal systems (where all of the derivations he makes from classical continuum systems are correct, or at least have their dimensional analysis done right, for which I will not bore you).  However, although Drexler does acknowledge that high-frequency quantum electronic devices will be extremely dissimilar to classical continuum models, I don’t think he mentions how much continuum models break down at the nanoscale. For example, he does mention that our macroscopic understanding of heat breaks down on the atomic level and that it works more like energy being transferred ballistically by phonons, for which the mean free path, in the absence of bounding surfaces, would exceed the dimensions of the structure in question. (This is also a concern when designing UHV chambers, because the mean free path for the handful of gas molecules inside can be many kilometers long or even more.) but then the conclusions feel uncertain to me. In particular, there’s a Table (2.1) that Drexler includes in this chapter, and I really dislike it because one of its main metrics is whether a physical quantity has a “good, moderate to poor, moderate to inapplicable, bad, good at small scales, or definitional” rating, and he doesn’t explain what those mean or how he assigned them beyond what appears to be "vibes.”.</p> 
<p>Further chapters consider more limits and edge cases. In particular, AC electrical circuits are different from these models. (They would be different because the failure modes for complex electromagnetic systems can be astounding: short circuiting, overloads, arc faults, insulation breakdown (which will be an issue when in Chapter 11 we discuss motor designs), harmonic distortion, voltage spikes or surges, etc.) </p> 
<p>Author’s Note: This is good, but I would almost say to scrap any continuum model used at the nanoscale, even if it dramatically increases the computational cost of our models. </p> 
<p>Conclusions: Scaling laws are bad for electromagnetic systems with small calculated time constants, reasonably good for thermal systems and slow-varying electromagnetic systems, and often excellent for mechanical systems as long as the component dimensions substantially exceed atomic dimensions. Scaling principles indicate that mechanical components can operate at high frequencies, accelerations, and power densities. The adverse scaling of wear lifetimes suggests that bearings are of special concern.</p> 

  <h1 id="ch3">Chapter 3: Potential Energy Surfaces</h1>   
  <p>Chapter Goal: This chapter talks about potential energy surfaces, which are fundamental to practical models of molecular structures and dynamics. </p> 
<p>Author’s Note: This chapter (and chapters 4–7, which cover various molecular dynamics methods and their implications) have made arguments of AI doom that rely on an AI system that’s able to “magically solve these problems computationally” seem significantly weaker to me. It’s not that I don’t think AI models for molecular systems could get significantly better (they could and they will), but for how many parts a macroscale nanodevice is made out of, this will require some serious experimentation in the loop and feedback testing. In the George Hotz vs. Eliezer Yudkowsky Youtube Debate (https://www.youtube.com/watch?v=6yQEA18C-XI), Eliezer takes the position that this is somewhat like a special case of the protein folding problem (bootstrapping APM) and makes an interesting point that it could maybe rely on the past corpus of experiments. However, due to how many approximations we have to make for our calculations, there is a high level of empiricism required for developing APM (computations are for invalidation rather than validation, and drug/material screening looks a lot like “a weak computation screens out 99%, the remaining 1% goes through a more rigorous computation that screens out 99% of that 1%, and then the remaining 0.01% has experiments run on it to find promising compounds) that Hotz seems to correctly understand. </p> 
<p>DEFINITION: Molecular potential energy surfaces, which “...stem from a visualization in which a potential energy function in N dimensions (that is, in configuration space) is described as a surface in N + 1 dimensions, with energy corresponding to height. (When N>2, the visualization is necessarily nonvisual.)”</p> 
<p>Visualization: significant features of a PES are its potential wells and the passes (cols) between them. “A stiff, stable structure resides in a well with steep walls and no low, accessible cols leading to alternative wells. A point representing the initial state of a chemically reactive structure, in contrast, resides in a well linked to another well by a col of accessible height. A point representing a mobile nanomechanical component commonly moves in a well with a long, level floor.” </p> 
<img src="images/Potential_Energy_Surface_for_Water.png" alt="water PES">
<p>(Potential Energy Surface for Water, https://en.wikipedia.org/wiki/Potential_energy_surface#/media/File:Potential_Energy_Surface_for_Water.png)</p> 
<p>HUGELY UNDERRATED POINT: Drexler makes an analogy saying that “molecular systems in which all transitions occur between distinct potential wells resemble transistor systems in which all transitions occur between distinct logic states: in both instances, the application of correct design principles can yield reliable behavior even though other systems subject to the same physical principles behave erratically.” </p> 
<p>I believe this analogy is a mostly valid one to make, but it is quite load-bearing: if we weren’t able to rely on these distinct transitions that were analogous to logic states, then all of this would fall apart. </p> 
<p>BIG CONCEPT: time vs. accuracy trade-off for simulations. You can have fast simulations or accurate simulations, but not fast, accurate simulations. </p> 
<p>Drexler doesn’t directly use wave equations or the mathematics of quantum mechanics here; that’s too low a level for the work we want to do. The goal is to think more like a chemist or materials scientist than a particle physicist. </p> 
<p>Quantum electrodynamics (QED) is a very accurate theory of electromagnetic fields and electrons that would be lovely to use if it weren’t so intractable for relevant systems. </p> 
<p>Schrodinger’s equation is also intractable for relevant systems. </p> 
<p>Why? Imagine you want to model a 1-D differential equation. If you put down 10 points to approximate it, your approximation will be pretty good. If you want to model a single atom in 3D space, you’ll need 10 points for each of the cartesian X, Y, and Z dimensions to model it, so it’ll be 10^3 points to consider. Now to model 300 atoms, you’ll need 10^(3 dimensions * 300 atoms) = 10^900 points to model the system, which is too much for any computer we have.</p> 
<p>Relativistic effects should be considered, but for valence electrons (especially for lighter elements), they're chemically negligible. Mostly. In 8.4.4b, Drexler discusses a case where that might not be the case, where relativistic effects in heavy atoms can cause strong spin-orbit coupling. </p> 
<p>Relevant: For chemistry and nanomechanical engineering, the full wave function gives more information than is necessary. </p> 
<p>Born-Oppenheimer Approximation: Treats the motion of electrons and nuclei separately. Nuclei are around 2,000 times bigger than electrons. If momentum = mass * velocity, to get the same velocity as a similar-moving electron, a nucleus would need 2,000 times the momentum. Thus, we can treat the nuclei as fixed compared to the electrons. </p> 
<p>Where does the Born-Oppenheimer approximation break down? When nuclear motions are fast (nuclear bombs), when excited states occur at very low energies (rare in stable molecules), and when small changes in nuclear coordinates cause large changes in the electron wave function (also rare). </p> 
<p>Potentially controversial statement: “In most nanomechanical systems, as in most chemical reactions, electron wave functions change smoothly with changes in molecular geometry.”</p> 
<p>Verdict: Not very controversial, but be careful for (literal) edge cases. </p> 
<p>Practical calculations require further approximations. </p> 
<p>The most popular approaches are known as molecular orbital methods. </p> 
<p>Independent-electron approximation: Treats each electron in a molecule as moving in an averaged field created by the other electrons, simplifying calculations.</p> 
<p>What’s the catch? This approximation neglects electron correlation, where the movement of one electron is influenced by the instantaneous position of others, potentially leading to significant errors in the description of electron behavior, especially in systems where electron-electron interactions are strong.</p> 
<p>Approximate wave functions:Approximate wave functions are used to solve the Schrödinger equation when exact solutions are not possible, using methods like Hartree-Fock to estimate electron distributions.</p> 
<p>What’s the catch? These approximations can fail to account for all electron correlations and might not accurately represent the true nature of the electron density, especially in excited states or systems with close-lying energy levels.</p> 
<p>Configuration Interaction (CI): improves upon the Hartree-Fock approximation by considering a superposition of multiple electron configurations, accounting for electron correlation more accurately.</p> 
<p>What’s the catch? CI can become computationally very demanding with increasing system size because the number of possible electron configurations grows exponentially, often making it impractical for large systems.</p> 
<p>Semiempirical methods (ab initio): use approximations and empirical parameters to simplify calculations, trading some accuracy for computational efficiency.</p> 
<p>What’s the catch? The empirical parameters may not transfer well between different systems or states, and the approximations can limit the method's predictive power, making these methods less reliable for systems outside the range for which the parameters were fitted.</p> 
<p>However, due to concerns about the expense, accuracy, and scalability of these methods, we can also look into molecular mechanics methods, which use molecular geometry instead of electronic structures for energy calculations. </p> 
<p>Fun fact: organic chemists represent molecular structures with ball-and-stick models: each ball is an atom, and each stick is a bond. </p> 
<p>For small molecules, the computational cost favors molecular mechanics over ab initio methods by a factor of 10^3. For larger systems, it can be even larger because molecular mechanics methods scale by the second or third power of the number of atoms (O(N^2) or O(N^3), ignoring some constant factor) rather than ab initio methods, which may be in the fourth or higher regimes (O(N^4)). </p> 
<p>A limitation of molecular mechanics is that it works best when the systems are not too far from equilibrium. I don’t think it will be much of an issue for, say, a solid block of atoms (that’s essentially just in one energetic state), but for modeling motors or moving objects, then molecular mechanics would likely not be a good choice. </p> 
<p>Thus, computations that aren’t essentially QED should be used more for invalidating certain designs than validating them. A simulation could tell you, “This won’t work,"  but only an experiment will tell you, “This will.”</p> 
<p>The MM2 model The current most-used force fields are either MM4 or are learned through machine learning models trained on quantum-chemical DFT data. (I'll show how MM2 and MM4 are different here.)</p> 
<p>Bond stretching: </p> 
<p>MM2 uses a simple harmonic oscillator model for bond stretching, considering the bond lengths and their deviations from equilibrium values.</p> 
<p>MM4 introduces anharmonic terms to better represent the potential energy associated with bond stretching, especially at larger deviations from the equilibrium bond length.</p> 
<p>Bond angle bending</p> 
<p>MM2 employs a quadratic function for angle bending, treating deviations from the equilibrium bond angle as a simple harmonic motion.</p> 
<p>MM4 incorporates more sophisticated functions, including anharmonic terms, to account for the energy changes associated with bond angle bending more accurately, especially for large deviations.</p> 
<p>Bond Torsion</p> 
<p>MM2: Models torsional interactions using a Fourier series or cosine functions to represent the energy variations with rotational angles of bond torsions.</p> 
<p>MM4 enhances the description of torsional potentials, adding terms to better capture the complexities of molecular conformational changes and stereoelectronic effects.</p> 
<p>Electrostatic Interactions</p> 
<p>MM2 calculates electrostatic interactions using Coulomb’s law, often with fixed partial charges on atoms.</p> 
<p>MM4: May include more advanced treatments of electrostatics, considering charge transfer, polarization, and possibly an improved charge distribution model.</p> 
<p>Nonbonded Interactions</p> 
<p>MM2: Accounts for van der Waals forces using Lennard-Jones potential or similar functions.</p> 
<p>MM4 includes explicit treatment of hydrogen bonding and improved van der Waals parameters.</p> 
<p>Complications and conjugated systems</p> 
<p>MM2: It doesn’t really capture some of the unusual subtleties of conjugated and aromatic systems or the effects of electron delocalization.</p> 
<p>MM4: Improved handling of conjugated systems, electron delocalization, and aromaticity, providing better energy and geometry predictions for complex organic molecules.</p> 
<p>So, overall, does the MM2 to MM4 switch change anything? It's hard to say, but I don’t think so. These simulations are still tools for disproving designs rather than proving designs. We need to run more experiments!</p> 
<p>Drexler also mentions additional terms that could be added to bonds under large tensile loads and nonbonded interactions under large compressive loads, but I believe that these are covered better in MM4, and ultimately, I am still skeptical of any computational model for such complex systems. </p> 
<p>(New Section) Potentials for chemical reactions </p> 
<p>Relationship to other methods: Molecular mechanics methods are based on the idea of structures with well-defined bonds; they cannot describe transformations that make or break bonds and cannot predict chemical instabilities. You could use CI methods to try and model bond cleavage and formation, but many papers also just rely on approximate potential surfaces.</p>  
<p>Bond cleavage and radical coupling: There’s not much interesting content here, just the basics: a simple reaction can be the bond cleavage of a dimer or a two-atom system that splits it into two radicals, but to combine two radicals into one, they would need paired, antiparallel electron spins to do bond formation. This doesn’t affect Drexler’s work very much; I’m not sure why he included this, to be honest. </p> 
<p>Abstraction reactions: Abstraction reactions are ones where reactions make and form bonds simultaneously, such as in the symmetrical hydrogen abstraction reaction: H3C + HCH3 -> H3CH + CH3. This doesn’t immediately relate to Drexler’s designs either, but I want to include as much information as I can. </p> 
 
<p>Continuum representation of surfaces: Van der Waals attractions between nanometer-scale objects can be substantial. (True!) Honestly, I think that using the Hamaker constant, which Drexler does, is typically used for continuum surfaces. </p> 
<p>Transverse-continuum models of surfaces: For non-metallic surfaces (which would undergo bonding rather than repulsion), we need different models for determining how two sliding, unreactive, atomically precise surfaces interact with each other. Drexler claims that if we neglect elastic deformation resulting from interfacial forces, it results in a surface-surface potential that could roughly look like “two planes rubbing against each other.” This is incorrect, as the surfaces on the atomic level would be much too small (although quite smooth enough) in order to make the assumption that we can neglect elastic deformation. What this implies is that we’ll need more computational power, obviously, but what designs now become unfeasible is uncertain. 3D bulk mechanosynthesis and motors should be relatively untouched, but rod logic computing and other methods that rely on surfaces sliding past each other should be looked at more carefully. There is the phenomenon of superlubricity (very low frictions at the atomic scale). Don’t think of superconductivity or superfluidity; the friction still exists; it’s just very small (like <0.01 for its kinetic coefficient of friction), which was theorized in Nanosystems and was experimentally shown to exist in 2004 (https://en.wikipedia.org/wiki/Superlubricity). That phenomenon could prove Drexler’s designs right, but I suspect that only the experiments will show how things really play out. </p> 
<p>Molecular models and bounded continuum models: For irregular surfaces, you really should be modeling at the level of interatomic potentials, but for bulk surfaces, Drexler suggests that one could try using bulk continuum models. This is developed more in Chapter 9, but I’m skeptical because these sorts of systems are so far out of the way of what exists even in nature (they’re much better than ribosomes, so don’t make that analogy), so I would just try to model all of the interatomic potentials if possible. </p> 
<p>Conclusions: The potential energy surface of a ground-state system can determine its mechanical properties, but (author’s note) I think that PES obviously isn’t enough to map everything out. For starters, everything is assumed to be stable, all transition states are mapped out, there are many approximations made, and many different surfaces may be needed for non-adiabatic events, but despite that, it’s a good starting point. Drexler does address some of these concerns later on (mostly the transition states and everything assumed to be stable), but I still have mixed feelings about using potential energy surfaces. </p> 
<p>Further reading: Drexler suggests additional resources for reading up on molecular quantum mechanics and potential energy surfaces, but honestly, I don’t think that will solve the problems inherent with trying to use computers to model these nanomechanical systems: in the end, you have to run the experiments. Working nanotechnology is still very much an empirical rather than a purely computational or theoretical discipline. </p>

  <h1 id="experimentalplan">Experimentation Plan</h1>  
  <img src="images/roughing_pump_and_feedthroughs.png" alt="roughing and feedthrough">
  <img src="images/diffusion_pump.png" alt="diffusion pump">
  <img src="images/bell_jar_1" alt="bell jar 1">
  <img src="images/wide_shot_bell_jar_2.png" alt="bell jar 2">
  <img src="images/basement1photo.png" alt="Basement 1 photo"> 
  <img src="images/basement2photo.png" alt="Basement 2 photo"> 
  <img src="images/basement3photo.png" alt="Basement 3 photo">  

  <h1 id="updates">Update Log</h1>  
  <p>Right now there are no updates besides minor cosmetics, but there will be many for the experimental section and then there will be an update for Volume 2.</p> 
  <p>If I make any egregious errors, I will credit you for finding it so that we can fix all of them.</p>

  
  <h1><a href="index.html">BACK TO HOME</a></h1>
</div>

</body>
</html>
